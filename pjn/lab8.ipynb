{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qR1eWH3owix"
   },
   "source": [
    "# Tworzenie zasobów\n",
    "\n",
    "Algorytmy wykorzystywane w problemach przetwarzania języka naturalnego opierają najczęściej swoje działanie o analizę dużych korpusów danych. O ile w zadaniach konkursowych często odpowiednie dane są już przygotowane, o tyle tworząc własne eksperymenty, często musimy sami pozyskać dane i przetransformować do użytecznej postaci.\n",
    "\n",
    "Dzisiejsze laboratoria dotyczyć będą tworzenia korpusów danych.\n",
    "\n",
    "## Automatyczne pozyskiwanie surowych danych tekstowych\n",
    "Dotychczas omawiane metody działały na surowym tekście, który transformowany był do odpowiedniej reprezentacji wektorowej (Bag of words, bag of ngrams, embeddingi). Jak zautomatyzować pozyskiwanie takich surowych danych z internetu?\n",
    "\n",
    "W tej części skupimy się na stworzeniu automatycznego pobieracza danych, który działać będzie w dwóch \"obszarach\":\n",
    "<ol>\n",
    "<li>crawler: moduł odwiedzający kolejne strony internetowy</li>\n",
    "<li>scraper: moduł ekstrahujący treść z konkretnych stron internetowych</li>\n",
    "</ol>\n",
    "\n",
    "Wykorzystajmy do tego dwie biblioteki: \n",
    "\n",
    "**urllib** - do odwiedzania stron\n",
    "\n",
    "**BeautifulSoup** - do parsowania danych (np. w formacie HTML).\n",
    "\n",
    "## Zadanie1: Napisz prosty ekstraktor danych ze stron WWW odwiedzający kilka podstron\n",
    "Ekstraktor ma odwiedzić zadaną stronę internetową, pobrać zawartość wszystkich tekstów wewnątrz paragrafów (wewnątrz tagów P zawartych w pobranym dokumencie HTML), a następnie odwiedzić 5 dowolnych linków z tej strony i z nich analogicznie pobrać zawartość.\n",
    "Łącznie powinniśmy otrzymać dane z 6 adresów internetowch (strona główna + 5 linków ze strony głównej).\n",
    "\n",
    "Do napisania crawlera przydać się mogą następujące funkcje:\n",
    "\n",
    "urllib.request.urlopen() - do pobrania zawartości strony\n",
    "findAll() na obiekcie BeautifulSoup, można ją wykorzystać do przeiterowania po wszystkich tagach danego rodzaju\n",
    "get_text() - Istnieje duża szansa, że wewnątrz tagów P znajdą się również inne tagi HTML, chcielibyśmy oczyścić \n",
    "z nich tekst. Można to zrobić albo z wyrażeniami regularnymi (robiliśmy takie zadanie na pierwszych laboratoriach!), albo użyć właśnie funkcji get_text() z BeautifulSoup\n",
    "\n",
    "Linki do dokumentacji:\n",
    "urllib, pobieranie danych: https://docs.python.org/3/howto/urllib2.html\n",
    "beautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ (przeczytanie QuickStart jest wystarczające do zrobienia tego zadania)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7020,
     "status": "ok",
     "timestamp": 1589793991631,
     "user": {
      "displayName": "Dariusz Max Adamski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhiuGg4VJ5se1vTg8oxApcXcd53c2pNERbwkfOf=s64",
      "userId": "05648663591110704065"
     },
     "user_tz": -120
    },
    "id": "OYIZqZnEowi0",
    "outputId": "d38c8d5c-7833-4fde-e1ab-fc119f926d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping https://news.ycombinator.com/news\n",
      "scraping https://news.ycombinator.com/newest\n",
      "scraping https://news.ycombinator.com/front\n",
      "scraping https://news.ycombinator.com/newcomments\n",
      "scraping https://news.ycombinator.com/ask\n",
      "[\"I'm not sure about that. HN is moderated very effectively. I couldn't comment on the workload there, not being a moderator.\", \"I'm leery of what you may be trying to imply here. This isn't some conspiracy thing, is it?\", \"They've decided not to have a stake in the future of our nation. They've proven themselves incapable of long-term planning beyond their own immediate gratification. They have no skin in the game.\", 'Why would we want them to vote? How could we possibly trust them to make responsible decisions with their track record?', \"If the devil offered them $10 cash on condition that the world is swallowed in flames in 100 years, what's to stop them?\", \"This comedy video of Health Secretary Matt Hancok has gone viral, but you can't laugh - it's painful to watch. These are the people in charge.\", 'https://www.youtube.com/watch?v=osgcTSVt7eU', 'But really, I want to contribute to the original project, and given I am not the original developer, it still relies on the them (and any other founders) to open up contributions. Fortunately, in this case, they have :)', \"Talking to working-class friends in the US about their access to healthcare. They feel concerned about the price of going to their doctor about things that our doctors urge us to talk to them about. Routine, expected medical treatment often takes years of saving up, and I've had friends die before they managed to get that treatment. When people wind up with an illness requiring ongoing treatment, any additional money they might make quickly goes down the drain.\", 'They also feel scared about losing their job, and therefore their access to healthcare, if they ever become ill enough to have to call off sick, thus compounding the problem.', \"Yes, it's rational when you're well-off to want to keep your money. But I'd very much prefer to make sure the people around me have real access to healthcare than that I can maybe buy a slightly nicer house.\", \"One way to start an open source business is to use a public/private licensing model, a.k.a. dual licensing with an 'open' license and a commercial license. The key is that you can have readable, modifiable, redistributable source code, without universally giving your work away for free. Instead, charge a fee for the right to use your software to create proprietary software, or for the right to use your software in a commercial context.\", \"License Zero is designed to support developers pursuing this kind of business model. There are two public licenses, Parity and Prosperity, and there's a payment platform to charge for private licenses. https://licensezero.com/\", 'Parity is a copy-left license, and is best suited to developer tools and software libraries. https://paritylicense.com/', 'Prosperity is a non-commercial license, and is best suited to end-user applications. https://prosperitylicense.com/', 'The Sustain podcast has an interview with the creator of License Zero, Kyle Mitchell. https://sustain.codefund.fm/29', 'We never received any negative feedback and were in high demand. Aside from that one super busy weekend, where things did get a bit cramped (but people knew in advance that it would be), nobody shared any beds or couches and couchsurfers had rooms separate from the hosts. I wouldn’t have had it any other way. Sharing beds with the host is... creepy (unless the person was given their own bed/couch and they mutually decided they wanted to share a bed, of course).', 'I miss couchsurfing too. I stopped being part of it around the time it got a sense for being a bit commercial. My circle of couchsurfing friends did it for the cultural exchange and at some point it drifted from that to just free accommodation and drinking parties. At least that’s the sense I got.', \"Not to  mention that you can literally post an RSS feed into pretty much every podcast app, it's an open ecosystem. Jesus.\", \"Can Google actually put people with some degree of sensibliity in charge of these decisions? This reminds me of Amazon deleting 1984 from people's Kindles.\", 'I can see there\\'s a whole ecosystem built up around kubernetes, cloud management and microservice meshes, for example.  You can set it all up for free using the FOSS offerings but it will sink a lot of time learning how to and maintaining the stack.  Or you can pay to use someone elses \"batteries included\"  distro which can be deployed with a few commands and save yourself a lot of time but spend $$$($..$??).', \"As long as projects don't actively refuse to accept contributions to the core,  respond well to issues, document stuff well and keep a nice pluggable architecture allowing other people to fork it and hack in their own features,  you're allowing people to choose between time or money.\", 'Benefits of having a \"pro\" version also filters down to the open core - if you\\'re wanting to market your product to businesses, you\\'re likely going to spend more effort on making it more user-friendly and therefore documentation and howto guides become more slick and comprehensive.', 'I\\'m so sick of hearing this \"argument\" against free speech. Yelling fire is not free speech. Never has been. Inciting hatred is however, and since you\\'re inciting hatred yourself, with wanting innocent people to die or throw them in jail, I hope you live in a country that has free speech laws, or you might be looking at jail time yourself.', 'Having hash line comments is probably a non-starter in a text format (the latex % syntax is probably a better idea indeed if you want line comments). Also the -- strikethrough -- syntax will probably conflict too much. You want comments to use a syntax which is otherwise nearly unused.', \">the 'rona\", \"Please just don't.\", 'Second, this is an index of all podcasts. If you are going to ban index searches google should also ban their own search engine because right now I can search for plenty of corona propaganda using it.']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_links(root, page):\n",
    "  res = [x['href'] for x in page.find_all('a', href=True)]\n",
    "  return [x if re.match(r'^https?:\\/\\/', x) else urljoin(root, x) for x in res if x != root]\n",
    "\n",
    "def get_paragraphs(page):\n",
    "  res = [x.get_text().strip() for x in page.find_all('p')]\n",
    "  return [x for x in res if x]\n",
    "  \n",
    "root = 'https://news.ycombinator.com'\n",
    "page = BeautifulSoup(urlopen(root).read())\n",
    "data = get_paragraphs(page)\n",
    "\n",
    "for url in get_links(root, page)[:5]:\n",
    "  print('scraping', url)\n",
    "  subpage = BeautifulSoup(urlopen(url).read())\n",
    "  data += get_paragraphs(subpage)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcumPFjDowi5"
   },
   "source": [
    "# Zadanie 2 - CONLL\n",
    "Dane ustrukturyzowane w formacie CONLL.\n",
    "\n",
    "Niektóre algorytmy korzystają z dodatkowych metadanych opisujących poszczególne tokeny (słowa). Bardzo popularnym formatem zapisu takich danych jest format CONLL. \n",
    "\n",
    "Reprezentacja CONLL polega na tym, że dany tekst dzielony jest na zdania, a następnie każde zdanie dzielone jest na tokeny (tokenizowane). Następnie dla każdego tokenu tworzymy listę opisującą cechy tego tokenu (słowa).\n",
    "Poniżej przykład wektora opisującego każdy token zadanego tekstu:\n",
    "<ol>\n",
    "    <li>ID - numer porządkowy tokenu w zdaniu</li>\n",
    "    <li>text - tekst tokenu w formie nieprzetworzonej</li>\n",
    "    <li>Part of Speech tag (POS tag) - informacja o części mowy, która powiązana jest z tym słowem </li>\n",
    "    <li>is digit - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest liczbą</li>\n",
    "    <li>is punct - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest znakiem interpunkcyjnym</li>\n",
    "</ol>\n",
    "\n",
    "Wektory cech dla kolejnych słów zapisywane są pod sobą. **Separatorem cech w wektorze jest pojedyncza spacja.**\n",
    "\n",
    "**Zdania zwyczajowo oddzielamy od siebie podwójnym znakiem nowej linii.**\n",
    "\n",
    "Historycznie CONLL był bardzo konkretnym formatem danych w którym mieliśmy z góry narzucone cechy (np. format CONLL-U https://universaldependencies.org/docs/format.html). Liczba cech ewoluowała jednak w czasie i w wielu miejscach CONLL stał się synonimem ogólnego formatu, w którym dobór cech zależy tylko od nas, jednak stałym jest zapis sekwencji tokenów jako sekwencji wierszy w tekście, gdzie każdy wiersz jest listą oddzielonych spacją wartości (cech), a zdania oddzielone są od siebie podwójnym znakiem nowej linii.\n",
    "\n",
    "\n",
    "### Przykład:\n",
    "\n",
    "Tekst: Kasia kupiła 2 lizaki: truskawkowy i zielony. Kasia używa Apple IPhone 5 i IPad.\n",
    "\n",
    "Reprezentacja CONLL **(spacje separujące kolumny zostały zwielokrotnione na potrzeby zwiększenia czytelności)**\n",
    "<pre>\n",
    "1 Kasia  RZECZOWNIK 0 0\n",
    "2 kupiła CZASOWNIK  0 0\n",
    "3 2      LICZEBNIK  1 0\n",
    "4 lizaki RZECZOWNIK 0 0\n",
    "5 .      _          0 1\n",
    "\n",
    "1 Kasia  RZECZOWNIK 0 0\n",
    "2 używa  CZASOWNIK  0 0\n",
    "3 Apple  RZECZOWNIK 0 0\n",
    "4 IPhone RZECZOWNIK 0 0\n",
    "5 5      LICZEBNIK  1 0\n",
    "6 i      SPÓJNIK    0 0\n",
    "7 iPad   RZECZOWNIK 0 0\n",
    "8 .      _          0 1\n",
    "</pre>\n",
    "\n",
    "**Zadanie**: Napisz funkcję, która z zadanego tekstu w formie surowego tekstu stworzy reprezentację CONLL opisaną wcześniej wymienionymi atrybutami (ID, text, POS-tag, is_digit, is_punct).\n",
    "\n",
    "Wykorzystaj sentence splitter i tokenizator z NLTK. Do uzyskania informacji o POS-tagach każdego tokenu wykorzystaj funkcję nltk.pos_tag(). W kolumnie związanej z POS-tagiem zapisz pos tag w takiej formie, w jakiej uzyskamy go z funkcji pos_tag (pos_tag() zwraca formy skrótowe, np. 'NN' dla rzeczowników), nie trzeba więc zamieniać napisu \"NN\" na \"RZECZOWNIK\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7781,
     "status": "ok",
     "timestamp": 1589793992411,
     "user": {
      "displayName": "Dariusz Max Adamski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhiuGg4VJ5se1vTg8oxApcXcd53c2pNERbwkfOf=s64",
      "userId": "05648663591110704065"
     },
     "user_tz": -120
    },
    "id": "x60YcdoY9GtV",
    "outputId": "0622c0a4-7c6e-4b9f-b19b-5555cc339c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7766,
     "status": "ok",
     "timestamp": 1589793992414,
     "user": {
      "displayName": "Dariusz Max Adamski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhiuGg4VJ5se1vTg8oxApcXcd53c2pNERbwkfOf=s64",
      "userId": "05648663591110704065"
     },
     "user_tz": -120
    },
    "id": "y62IbEIQowi6",
    "outputId": "1d35f775-4361-42da-b96e-0e669fc9a94b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Kate NNP 0 0\n",
      "2 uses JJ 0 0\n",
      "3 IPhone PRP 0 0\n",
      "4 5 CD 1 0\n",
      "5 and DT 0 0\n",
      "6 IPad PRP 0 0\n",
      "7 . _ 0 1\n",
      "\n",
      "1 Kate NNP 0 0\n",
      "2 bought NN 0 0\n",
      "3 2 CD 1 0\n",
      "4 lolipops NN 0 0\n",
      "5 . _ 0 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def generate_conll(text):\n",
    "  for sentence in sent_tokenize(text):\n",
    "    for i, word in enumerate(word_tokenize(sentence)):\n",
    "      tag = pos_tag(word)[0][1]\n",
    "      is_digit = int(tag == 'CD')\n",
    "      is_punct = int(tag in ',.')\n",
    "      if is_punct: tag = '_'\n",
    "      print(i+1, word, tag, is_digit, is_punct)\n",
    "    print()\n",
    "\n",
    "generate_conll(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolipops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "n8ZzaqIvowi-"
   },
   "source": [
    "\n",
    "Wyobraźmy sobie teraz, że chcielibyśmy wykrywać wzmianki o urządzeniach elektronicznych w tekście. W jaki sposób zakodować informację o (potencjalnie wielotokenowych) nazwach produktów w CONLL, tak, aby później móc wykonać proces uczenia?\n",
    "\n",
    "Dodajmy w naszym CONLLu dodatkową kolumnę reprezentującą informację o urządzeniach elektronicznych.\n",
    "Nazwy urządzeń mogą składać się potencjalnie z wielu słów.\n",
    "Do zakodowania wielotokenowych tekstów używa się najczęściej notacji IOB, gdzie każda literka skrótu oznacza interpretację aktualnego słowa:\n",
    "<ul>\n",
    "    <li> B = begin, marker, który mówi, że aktualne słowo to początek nazwy </li>\n",
    "    <li> I = inside, marker, który mówi, że aktualne słowo to kontynacja nazwy, która rozpoczyna się wystąpieniem wcześniejszego B</li>\n",
    "    <li> O = outside, marker, który mówi, że aktualne słowo nie jest interesującą nas nazwą (jest poza nią) </li>\n",
    "</ul>\n",
    "\n",
    "Po dodaniu nowej kolumny (na końcu) nasz CONLL przybiera postać:\n",
    "\n",
    "<pre>\n",
    "1 Kasia  RZECZOWNIK 0 0 O\n",
    "2 kupiła CZASOWNIK  0 0 O\n",
    "3 2                 1 0 O\n",
    "4 lizaki RZECZOWNIK 0 0 O\n",
    "5 .      _          0 1 O\n",
    "\n",
    "1 Kasia  RZECZOWNIK 0 0 O\n",
    "2 używa             0 0 O\n",
    "3 Apple  RZECZOWNIK 0 0 B\n",
    "4 IPhone RZECZOWNIK 0 0 I\n",
    "5 5                 1 0 I\n",
    "6 i      SPÓJNIK    0 0 O\n",
    "7 iPad   RZECZOWNIK 0 0 B\n",
    "8 .      _          0 1 0\n",
    "</pre>\n",
    "\n",
    "Zwróćcie Państwo uwagę na ostatnią kolumnę, czytając tekst od góry w dół, wystąpienie literki \"B\" oznacza początek interesującej frazy (Apple), jeśli zaraz za \"B\" pojawia się sekwencja oznaczona jako \"I\" - kolejne tokeny stanowią kontynuację interesującej nas frazy, w tym przypadku 3 tokeny \"Apple IPhone 5\" tworzą jeden byt. Poza tym widzimy, że \"iPad\" stanowi osobny, jednotokenowy byt.\n",
    "\n",
    "Po co rozróżniać pomiędzy \"B\", \"I\" i \"O\", czy nie można uwzględnić tylko dwóch tagów \"wewnątrz frazy\", \"poza frazą\"? Teoretycznie można, ale wprowadzimy w ten sposób sytuacje niejednoznaczne. \n",
    "\n",
    "Sprawdźmy to na przykładzie sekwencji \"XBox Playstation\" reprezentującej 2 osobne byty. Używając tagowania IOB nasza sekwencja wyglądałaby tak:\n",
    "\n",
    "XBox B\n",
    "PlayStation B\n",
    "\n",
    "Widzimy więc, że dwa tagi \"B\" oznaczają dwa początki osobnych fraz. Co jednak gdybyśmy używali tagów \"wewnątrz (interesującej nas) frazy\", \"poza (interesującą nas) frazą\"?\n",
    "\n",
    "XBox \"wewnątrz (interesującej nas) frazy\"\n",
    "Playstation \"wewnątrz (interesującej nas) frazy\"\n",
    "\n",
    "W tej sytuacji oznaczyliśmy poprawnie oba tokeny jako części interesujących nas fraz. Jednak nie wiemy, czy XBox Playstation to jedna, czy dwie osobne frazy (byty) -- stąd format IOB jest zdecydowanie bezpieczniejszym wyborem.\n",
    "\n",
    "**Zadanie**: Napisz funkcję, która wygeneruje CONLL z uwzględnieniem tagów IOB dotyczących urządzeń.\n",
    "Nasza funkcja posiada teraz dodatkowy argument devices, który zawiera listę obiektów, które opisują gdzie (przesunięcie znakowe) znajduje się początek i koniec wzmianek.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTCAXqI0-xAk"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1589794224799,
     "user": {
      "displayName": "Dariusz Max Adamski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhiuGg4VJ5se1vTg8oxApcXcd53c2pNERbwkfOf=s64",
      "userId": "05648663591110704065"
     },
     "user_tz": -120
    },
    "id": "MJtMRaMIowi_",
    "outputId": "78bf6fbc-f8d6-4fc3-e114-c50139e70ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Kate PROPN 0 0 O\n",
      "2 uses VERB 0 0 O\n",
      "3 IPhone PROPN 0 0 B\n",
      "4 5 _ 1 0 I\n",
      "5 and CCONJ 0 0 O\n",
      "6 IPad PROPN 0 0 B\n",
      "7 . _ 0 1 O\n",
      "\n",
      "1 Kate PROPN 0 0 O\n",
      "2 bought VERB 0 0 O\n",
      "3 2 _ 1 0 O\n",
      "4 lolipops NOUN 0 0 O\n",
      "5 . _ 0 1 O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_CONLL(text, devices=[]):\n",
    "  for sentence in nlp(text).sents:\n",
    "    for i, token in enumerate(sentence):\n",
    "\n",
    "      iob = 'O'\n",
    "      if devices:\n",
    "        begin, end = devices[0]['begin'], devices[0]['end']\n",
    "        if begin == token.idx:\n",
    "            iob = 'B'\n",
    "        elif begin < token.idx < end:\n",
    "            iob = 'I'\n",
    "        if end <= token.idx + len(token):\n",
    "          devices.pop(0)\n",
    "  \n",
    "      is_digit, is_punct = int(token.is_digit), int(token.is_punct)\n",
    "      pos = '_' if is_digit or is_punct else token.pos_\n",
    "      print(i+1, token, pos, is_digit, is_punct, iob)\n",
    "    print()\n",
    "    \n",
    "generate_CONLL(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolipops.\", devices=[{\"begin\": 10, \"end\":18}, {\"begin\": 23, \"end\": 27}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRRttijnowjD"
   },
   "source": [
    "Często chcemy w tekście naraz oznaczać byty, które należą do różnych kategorii, np. lokacje, numery telefonów, daty, wzmianki o osobach. W takich sytuacjach używa się również kodowania IOB jednak wzbogaca się etykiety o odpowiednie informacje używając formatu:\n",
    "\n",
    "{tag IOB}-{etykieta kategorii}\n",
    "\n",
    "Stąd daty przyjmują oznaczenia: B-DATE / I-DATE, osoby B-PERSON / I-PERSON, numery telefonów B-PHONENUMBER / I-PHONENUMBER, lokacje: B-LOCATION / I-LOCATION itp. Wiemy zatem czy dany token należy do interesującej nas frazy i do jakiej kategorii przypisana jest ta fraza."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab_8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
