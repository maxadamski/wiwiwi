
@article{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {This work proposes a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the imageembedding, and shows that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	journal = {ArXiv},
	author = {Ramesh, A. and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	year = {2022},
}

@article{eberhart_completion_2020,
	title = {Completion {Reasoning} {Emulation} for the {Description} {Logic} {EL}+},
	abstract = {This work presents a new approach to integrating deep learning with knowledge-based systems that shows promise and demonstrates that this idea is feasible by training a long short-term memory artificial neural network to learn EL+ reasoning patterns with two different data sets. We present a new approach to integrating deep learning with knowledge-based systems that we believe shows promise. Our approach seeks to emulate reasoning structure, which can be inspected part-way through, rather than simply learning reasoner answers, which is typical in many of the black-box systems currently in use. We demonstrate that this idea is feasible by training a long short-term memory (LSTM) artificial neural network to learn EL+ reasoning patterns with two different data sets. We also show that this trained system is resistant to noise by corrupting a percentage of the test data and comparing the reasoner's and LSTM's predictions on corrupt data with correct answers.},
	journal = {AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering},
	author = {Eberhart, Aaron and Ebrahimi, Monireh and Zhou, Lu and Shimizu, C. and Hitzler, P.},
	year = {2020},
}

@inproceedings{bordes_translating_2013,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'13},
	title = {Translating embeddings for modeling multi-relational data},
	abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
	urldate = {2022-07-04},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {Curran Associates Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Durán, Alberto and Weston, Jason and Yakhnenko, Oksana},
	month = dec,
	year = {2013},
	pages = {2787--2795},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	abstract = {GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	journal = {NeurIPS},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, J. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, T. and Child, Rewon and Ramesh, A. and Ziegler, Daniel M. and Wu, Jeff and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	abstract = {The objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert \& Weston, 2008; Mnih \& Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a “better” basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).},
	booktitle = {{AISTATS}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = {2010},
}

@article{potoniec_inductive_2022,
	title = {Inductive {Learning} of {OWL} 2 {Property} {Chains}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9724179/},
	doi = {10.1109/ACCESS.2022.3155816},
	urldate = {2022-07-04},
	journal = {IEEE Access},
	author = {Potoniec, Jedrzej},
	year = {2022},
	pages = {25327--25340},
}

@article{masters_revisiting_2018,
	title = {Revisiting {Small} {Batch} {Training} for {Deep} {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1804.07612},
	doi = {10.48550/ARXIV.1804.07612},
	abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size \$m\$. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between \$m = 2\$ and \$m = 32\$, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
	urldate = {2022-06-30},
	journal = {arXiv:1804.07612 [cs, stat]},
	author = {Masters, Dominic and Luschi, Carlo},
	year = {2018},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{sarker_efficient_2019,
	title = {Efficient {Concept} {Induction} for {Description} {Logics}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4161},
	doi = {10.1609/aaai.v33i01.33013036},
	abstract = {Concept Induction refers to the problem of creating complex Description Logic class descriptions (i.e., TBox axioms) from instance examples (i.e., ABox data). In this paper we look particularly at the case where both a set of positive and a set of negative instances are given, and complex class expressions are sought under which the positive but not the negative examples fall. Concept induction has found applications in ontology engineering, but existing algorithms have fundamental performance issues in some scenarios, mainly because a high number of invokations of an external Description Logic reasoner is usually required. In this paper we present a new algorithm for this problem which drastically reduces the number of reasoner invokations needed. While this comes at the expense of a more limited traversal of the search space, we show that our approach improves execution times by up to several orders of magnitude, while output correctness, measured in the amount of correct coverage of the input instances, remains reasonably high in many cases. Our approach thus should provide a strong alternative to existing systems, in particular in settings where other systems are prohibitively slow.},
	language = {en},
	number = {01},
	urldate = {2022-07-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sarker, Md Kamruzzaman and Hitzler, Pascal},
	month = jul,
	year = {2019},
	pages = {3036--3043},
}

@article{tharwat_classification_2021,
	title = {Classification assessment methods},
	volume = {17},
	issn = {2634-1964, 2210-8327},
	url = {https://www.emerald.com/insight/content/doi/10.1016/j.aci.2018.08.003/full/html},
	doi = {10.1016/j.aci.2018.08.003},
	abstract = {Classification techniques have been applied to many applications in various fields of sciences. There are several ways of evaluating classification algorithms. The analysis of such metrics and its significance must be interpreted correctly for evaluating different learning algorithms. Most of these measures are scalar metrics and some of them are graphical methods. This paper introduces a detailed overview of the classification assessment measures with the aim of providing the basics of these measures and to show how it works to serve as a comprehensive source for researchers who are interested in this field. This overview starts by highlighting the definition of the confusion matrix in binary and multi-class classification problems. Many classification measures are also explained in details, and the influence of balanced and imbalanced data on each metric is presented. An illustrative example is introduced to show (1) how to calculate these measures in binary and multi-class classification problems, and (2) the robustness of some measures against balanced and imbalanced data. Moreover, some graphical measures such as Receiver operating characteristics (ROC), Precision-Recall, and Detection error trade-off (DET) curves are presented with details. Additionally, in a step-by-step approach, different numerical examples are demonstrated to explain the preprocessing steps of plotting ROC, PR, and DET curves.},
	language = {en},
	number = {1},
	urldate = {2022-07-04},
	journal = {Applied Computing and Informatics},
	author = {Tharwat, Alaa},
	month = jan,
	year = {2021},
	pages = {168--192},
}

@misc{noauthor_owl_2012,
	title = {{OWL} 2 {Web} {Ontology} {Language} {Structural} {Specification} and {Functional}-{Style} {Syntax} ({Second} {Edition})},
	url = {https://www.w3.org/TR/owl2-syntax/},
	urldate = {2022-02-04},
	year = {2012},
}

@misc{noauthor_owl_2004,
	title = {{OWL} {Web} {Ontology} {Language} {Overview}},
	url = {https://www.w3.org/TR/owl-features/},
	urldate = {2022-06-29},
	year = {2004},
}

@incollection{baader_chapter_2008,
	title = {Chapter 3 {Description} {Logics}},
	volume = {3},
	isbn = {9780444522115},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574652607030039},
	language = {en},
	urldate = {2022-06-29},
	booktitle = {Foundations of {Artificial} {Intelligence}},
	publisher = {Elsevier},
	author = {Baader, Franz and Horrocks, Ian and Sattler, Ulrike},
	year = {2008},
	doi = {10.1016/S1574-6526(07)03003-9},
	pages = {135--179},
}

@incollection{rudolph_foundations_2011,
	address = {Berlin, Heidelberg},
	title = {Foundations of {Description} {Logics}},
	volume = {6848},
	isbn = {9783642230318 9783642230325},
	url = {http://link.springer.com/10.1007/978-3-642-23032-5_2},
	urldate = {2022-05-31},
	booktitle = {Reasoning {Web}. {Semantic} {Technologies} for the {Web} of {Data}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rudolph, Sebastian},
	editor = {Polleres, Axel and d’Amato, Claudia and Arenas, Marcelo and Handschuh, Siegfried and Kroner, Paula and Ossowski, Sascha and Patel-Schneider, Peter},
	year = {2011},
	doi = {10.1007/978-3-642-23032-5_2},
	pages = {76--136},
}

@article{clevert_fast_2016,
	title = {Fast and {Accurate} {Deep} {Network} {Learning} by {Exponential} {Linear} {Units} ({ELUs})},
	abstract = {The "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies and significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
	journal = {ICLR},
	author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, S.},
	year = {2016},
}

@book{benson_principles_2010,
	address = {London},
	series = {Health {Informatics}},
	title = {Principles of {Health} {Interoperability} {HL7} and {SNOMED}},
	isbn = {9781848828025 9781848828032},
	url = {http://link.springer.com/10.1007/978-1-84882-803-2},
	urldate = {2022-06-29},
	publisher = {Springer London},
	author = {Benson, Tim},
	year = {2010},
	doi = {10.1007/978-1-84882-803-2},
}

@incollection{goos_inductive_1996,
	address = {Berlin, Heidelberg},
	title = {Inductive learning in symbolic domains using structure-driven recurrent neural networks},
	volume = {1137},
	isbn = {9783540617082 9783540706694},
	url = {http://link.springer.com/10.1007/3-540-61708-6_60},
	urldate = {2022-06-28},
	booktitle = {{KI}-96: {Advances} in {Artificial} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Küchler, Andreas and Goller, Christoph},
	editor = {Goos, G. and Hartmanis, J. and Leeuwen, J. and Carbonell, Jaime G. and Siekmann, Jörg and Görz, Günther and Hölldobler, Steffen},
	year = {1996},
	doi = {10.1007/3-540-61708-6_60},
	pages = {183--197},
}

@article{jackson_robot_2019,
	title = {{ROBOT}: {A} {Tool} for {Automating} {Ontology} {Workflows}},
	volume = {20},
	issn = {1471-2105},
	shorttitle = {{ROBOT}},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3002-3},
	doi = {10.1186/s12859-019-3002-3},
	language = {en},
	number = {1},
	urldate = {2022-06-28},
	journal = {BMC Bioinformatics},
	author = {Jackson, Rebecca C. and Balhoff, James P. and Douglass, Eric and Harris, Nomi L. and Mungall, Christopher J. and Overton, James A.},
	month = dec,
	year = {2019},
	pages = {407},
}

@article{sirin_pellet_2007,
	title = {Pellet: {A} practical {OWL}-{DL} reasoner},
	volume = {5},
	issn = {15708268},
	shorttitle = {Pellet},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1570826807000169},
	doi = {10.1016/j.websem.2007.03.004},
	language = {en},
	number = {2},
	urldate = {2022-06-28},
	journal = {Journal of Web Semantics},
	author = {Sirin, Evren and Parsia, Bijan and Grau, Bernardo Cuenca and Kalyanpur, Aditya and Katz, Yarden},
	month = jun,
	year = {2007},
	pages = {51--53},
}

@article{towell_knowledge-based_1994,
	title = {Knowledge-based artificial neural networks},
	volume = {70},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370294901058},
	doi = {10.1016/0004-3702(94)90105-8},
	language = {en},
	number = {1-2},
	urldate = {2022-06-27},
	journal = {Artificial Intelligence},
	author = {Towell, Geoffrey G. and Shavlik, Jude W.},
	month = oct,
	year = {1994},
	pages = {119--165},
}

@article{yang_embedding_2015,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	abstract = {It is found that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. Abstract: We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) =\&gt; Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	journal = {ICLR},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, L.},
	year = {2015},
}

@article{bianchi_capabilities_2019,
	title = {On the {Capabilities} of {Logic} {Tensor} {Networks} for {Deductive} {Reasoning}},
	url = {https://www.semanticscholar.org/paper/On-the-Capabilities-of-Logic-Tensor-Networks-for-Bianchi-Hitzler/9d8a3adeb4ad8e36c399ba19bb1220ecb73fb56b},
	abstract = {Semantic Scholar extracted view of "On the Capabilities of Logic Tensor Networks for Deductive Reasoning" by Federico Bianchi et al.},
	language = {en},
	urldate = {2022-06-27},
	journal = {AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering},
	author = {Bianchi, Federico and Hitzler, P.},
	year = {2019},
}

@techreport{besold_neural-symbolic_2017,
	title = {Neural-{Symbolic} {Learning} and {Reasoning}: {A} {Survey} and {Interpretation}},
	shorttitle = {Neural-{Symbolic} {Learning} and {Reasoning}},
	url = {http://arxiv.org/abs/1711.03902},
	abstract = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
	number = {arXiv:1711.03902},
	urldate = {2022-06-27},
	institution = {arXiv},
	author = {Besold, Tarek R. and Garcez, Artur d'Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
	month = nov,
	year = {2017},
	note = {arXiv:1711.03902 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{saito_precision-recall_2015,
	title = {The {Precision}-{Recall} {Plot} {Is} {More} {Informative} than the {ROC} {Plot} {When} {Evaluating} {Binary} {Classifiers} on {Imbalanced} {Datasets}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0118432},
	doi = {10.1371/journal.pone.0118432},
	language = {en},
	number = {3},
	urldate = {2022-06-27},
	journal = {PLOS ONE},
	author = {Saito, Takaya and Rehmsmeier, Marc},
	editor = {Brock, Guy},
	month = mar,
	year = {2015},
	pages = {e0118432},
}

@article{fawcett_introduction_2006,
	title = {An introduction to {ROC} analysis},
	volume = {27},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016786550500303X},
	doi = {10.1016/j.patrec.2005.10.010},
	language = {en},
	number = {8},
	urldate = {2022-06-24},
	journal = {Pattern Recognition Letters},
	author = {Fawcett, Tom},
	month = jun,
	year = {2006},
	pages = {861--874},
}

@book{hitzler_foundations_2009,
	edition = {0},
	title = {Foundations of {Semantic} {Web} {Technologies}},
	isbn = {9781420090512},
	url = {https://www.taylorfrancis.com/books/9781420090512},
	language = {en},
	urldate = {2022-06-24},
	publisher = {Chapman and Hall/CRC},
	author = {Hitzler, Pascal and Krotzsch, Markus and Rudolph, Sebastian},
	month = aug,
	year = {2009},
	doi = {10.1201/9781420090512},
}

@article{bednarek_robustness_2020,
	title = {On {Robustness} of {Multi}-{Modal} {Fusion}—{Robotics} {Perspective}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/9/7/1152},
	doi = {10.3390/electronics9071152},
	abstract = {The efficient multi-modal fusion of data streams from different sensors is a crucial ability that a robotic perception system should exhibit to ensure robustness against disturbances. However, as the volume and dimensionality of sensory-feedback increase it might be difficult to manually design a multimodal-data fusion system that can handle heterogeneous data. Nowadays, multi-modal machine learning is an emerging field with research focused mainly on analyzing vision and audio information. Although, from the robotics perspective, haptic sensations experienced from interaction with an environment are essential to successfully execute useful tasks. In our work, we compared four learning-based multi-modal fusion methods on three publicly available datasets containing haptic signals, images, and robots’ poses. During tests, we considered three tasks involving such data, namely grasp outcome classification, texture recognition, and—most challenging—multi-label classification of haptic adjectives based on haptic and visual data. Conducted experiments were focused not only on the verification of the performance of each method but mainly on their robustness against data degradation. We focused on this aspect of multi-modal fusion, as it was rarely considered in the research papers, and such degradation of sensory feedback might occur during robot interaction with its environment. Additionally, we verified the usefulness of data augmentation to increase the robustness of the aforementioned data fusion methods.},
	language = {en},
	number = {7},
	urldate = {2022-06-23},
	journal = {Electronics},
	author = {Bednarek, Michal and Kicki, Piotr and Walas, Krzysztof},
	month = jul,
	year = {2020},
	keywords = {machine learning, multi-modal fusion, robotics},
	pages = {1152},
}

@article{lamy_owlready_2017,
	title = {Owlready: {Ontology}-oriented programming in {Python} with automatic classification and high level constructs for biomedical ontologies},
	volume = {80},
	issn = {09333657},
	shorttitle = {Owlready},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365717300271},
	doi = {10.1016/j.artmed.2017.07.002},
	language = {en},
	urldate = {2022-06-21},
	journal = {Artificial Intelligence in Medicine},
	author = {Lamy, Jean-Baptiste},
	month = jul,
	year = {2017},
	pages = {11--28},
}

@article{musen_protege_2015,
	title = {The protégé project: a look back and a look forward},
	shorttitle = {The protégé project},
	doi = {10.1145/2757001.2757003},
	abstract = {Protege has become the most widely used software for building and maintaining ontologies, and the Web-based version has become extremely popular, and it recently exceeded the desktop-client in its degree of usage. Few funded research projects in computer science go on for decades. It’s even questionable whether any project should be sustained that long. Nevertheless, the Protege project at Stanford University began in the 1980s and is still going strong, helping developers to construct reusable ontologies and to build knowledge-based systems. The recognition of our work at the International Semantic Web Conference in October 2014 with the “Ten Years” Award was a great honor. The award has provided an opportunity f o r reflection—both on the Protege project itself and on the need for computational infrastructure in the AI community. 
 
Protege has become the most widely used software for building and maintaining ontologies. It is by no means the only solution, and there are known problems with Protege, but we appreciate that the system is extremely popular. Although his study is now a bit dated, Cardoso (2007) surveyed the Semantic Web community and found that two-thirds of his respondents used Protege. To date, more than 250,000 people have registered to use the software. Many Fortune 500 companies use Protege to build their ontologies. Important government projects, such as the development of the National Cancer Institute Thesaurus (Noy et al., 2008; Figure 1) and the World Health Organization’s International Classification of Diseases (ICD-11; Tudorache et al., 2010) depend on the software. 
 
 
 
Figure 1 
 
The Protege 5 Desktop System. The figure shows the most recent version of the Protege system used to edit the National Cancer Institute Thesaurus. In the Figure, the user has selected the class Antigen Gene. The visualization ... 
 
 
 
Protege currently exists in a variety of frameworks. A desktop system (Protege 5) supports many advanced features to enable the construction and management of OWL ontologies (see Figure 1). A Web-based system (WebProtege) offers distributed access over the Internet using any Web browser and, by design, is much simpler to use for many ontology-engineering tasks (Figure 2). The Web-based version has become extremely popular, and it recently exceeded the desktop-client in its degree of usage. It is extremely handy to be able to point a Web browser to an appropriate server and to begin editing. Like a Google doc, a WebProtege ontology can be easily shared with a distributed group of users who can engage in collaborative authoring activities from wherever they happen to be logged in. The development environment used by the World Health Organization to manage ICD-11 is based on WebProtege (Tudorache et al., 2010). 
 
 
 
Figure 2 
 
WebProtege. The Web-based version of Protege offers users and their collaborators the opportunity to share and edit ontologies online, much like a Google doc. Here we see the Ontology for Parasite Lifecycle (OPL), an ontology ... 
 
 
 
Older versions of the Protege desktop system have included support for editing ontologies represented in a frame language (namely, in the OKBC framework developed by the DARPA Knowledge Sharing Initiative in the 1990s; Neches et al., 1991). Comparable functionality has not yet been migrated to current versions of Protege, however. All versions of Protege may be downloaded from the project’s Web site (http://protege.stanford.edu). They are available under an open-source license. 
 
The paper for which the Protege team won the “Ten Years” Award (Knublauch et al., 2004) describes the first Protege system to support the World Wide Web Consortium’s recommended Web Ontology Language (OWL). We had been tracking the emerging OWL specification, and Holger Knublauch, then a post-doctoral fellow in our laboratory, worked with the rest of the team to extend Protege to create what was, at the time, the only ontology-development platform that could accommodate nearly the complete OWL specification. Protege’s support for OWL has been enhanced over the years, particularly through a very successful collaboration with Alan Rector’s CO-ODE project at the University of Manchester, and recent versions of Protege fully support the latest OWL 2.0 specification. 
 
When people think of Protege, they think of an editor for ontologies, and they think of OWL. Protege did not start out this way, however. Gennari et al. (2003) traced the history of the first 15 years of the Protege project, documenting the many shifts in perspective as Protege progressed from a student project for a Ph.D. dissertation that focused on problems of building knowledge-based systems (Musen, 1989) to a major open-source platform supported by a huge community of diverse users (Musen, 2005).},
	journal = {SIGAI},
	author = {Musen, M.},
	year = {2015},
}

@article{mcinnes_umap_2020,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
	shorttitle = {{UMAP}},
	url = {http://arxiv.org/abs/1802.03426},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	urldate = {2022-06-20},
	journal = {arXiv:1802.03426 [cs, stat]},
	author = {McInnes, Leland and Healy, John and Melville, James},
	month = sep,
	year = {2020},
	note = {arXiv: 1802.03426},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{baader_introduction_2017,
	address = {Cambridge},
	title = {An {Introduction} to {Description} {Logic}},
	isbn = {9781139025355},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139025355},
	urldate = {2022-05-31},
	publisher = {Cambridge University Press},
	author = {Baader, Franz and Horrocks, Ian and Lutz, Carsten and Sattler, Uli},
	year = {2017},
	doi = {10.1017/9781139025355},
}

@article{he_outer_2018,
	title = {Outer {Product}-based {Neural} {Collaborative} {Filtering}},
	url = {http://arxiv.org/abs/1808.03912},
	abstract = {In this work, we contribute a new multi-layer neural network architecture named ONCF to perform collaborative filtering. The idea is to use an outer product to explicitly model the pairwise correlations between the dimensions of the embedding space. In contrast to existing neural recommender models that combine user embedding and item embedding via a simple concatenation or element-wise product, our proposal of using outer product above the embedding layer results in a two-dimensional interaction map that is more expressive and semantically plausible. Above the interaction map obtained by outer product, we propose to employ a convolutional neural network to learn high-order correlations among embedding dimensions. Extensive experiments on two public implicit feedback data demonstrate the effectiveness of our proposed ONCF framework, in particular, the positive effect of using outer product to model the correlations between embedding dimensions in the low level of multi-layer neural recommender model. The experiment codes are available at: https://github.com/duxy-me/ConvNCF},
	urldate = {2022-05-28},
	journal = {arXiv:1808.03912 [cs, stat]},
	author = {He, Xiangnan and Du, Xiaoyu and Wang, Xiang and Tian, Feng and Tang, Jinhui and Chua, Tat-Seng},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.03912},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{socher_recursive_2013,
	address = {Seattle, Washington, USA},
	title = {Recursive {Deep} {Models} for {Semantic} {Compositionality} {Over} a {Sentiment} {Treebank}},
	url = {https://aclanthology.org/D13-1170},
	urldate = {2022-05-17},
	booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
	month = oct,
	year = {2013},
	pages = {1631--1642},
}

@article{chen_contextual_2022,
	title = {Contextual {Semantic} {Embeddings} for {Ontology} {Subsumption} {Prediction}},
	url = {http://arxiv.org/abs/2202.09791},
	abstract = {Automating ontology curation is a crucial task in knowledge engineering. Prediction by machine learning techniques such as semantic embedding is a promising direction, but the relevant research is still preliminary. In this paper, we present a class subsumption prediction method named BERTSubs, which uses the pre-trained language model BERT to compute contextual embeddings of the class labels and customized input templates to incorporate contexts of surrounding classes. The evaluation on two large-scale real-world ontologies has shown its better performance than the state-of-the-art.},
	urldate = {2022-04-20},
	journal = {arXiv:2202.09791 [cs]},
	author = {Chen, Jiaoyan and He, Yuan and Jimenez-Ruiz, Ernesto and Dong, Hang and Horrocks, Ian},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.09791},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{robins_catastrophic_1995,
	title = {Catastrophic {Forgetting}, {Rehearsal} and {Pseudorehearsal}},
	volume = {7},
	issn = {0954-0091},
	url = {https://doi.org/10.1080/09540099550039318},
	doi = {10.1080/09540099550039318},
	abstract = {This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. We replicate some of the experiments described by Ratcliff (1990), including those relating to a simple 'recency' based rehearsal regime. We then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, 'sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. We describe a solution to this problem, 'pseudorehearsal', a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old information.},
	number = {2},
	urldate = {2022-04-02},
	journal = {Connection Science},
	author = {Robins, Anthony},
	month = jun,
	year = {1995},
	keywords = {Catastrophic Forgetting;Catastrophic Interference;Stability;Plasticity;Rehearsal.},
	pages = {123--146},
}

@inproceedings{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	abstract = {This work proposes a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function, and provides empirical evidence that this modification substantially improves Adam's generalization performance. L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL},
	booktitle = {{ICLR}},
	author = {Loshchilov, I. and Hutter, F.},
	year = {2019},
}

@inproceedings{glimm_optimising_2010,
	address = {Berlin, Heidelberg},
	title = {Optimising {Ontology} {Classification}},
	isbn = {9783642177460},
	doi = {10.1007/978-3-642-17746-0_15},
	abstract = {Ontology classification—the computation of subsumption hierarchies for classes and properties—is one of the most important tasks for OWL reasoners. Based on the algorithm by Shearer and Horrocks [9], we present a new classification procedure that addresses several open issues of the original algorithm, and that uses several novel optimisations in order to achieve superior performance. We also consider the classification of (object and data) properties. We show that algorithms commonly used to implement that task are incomplete even for relatively weak ontology languages. Furthermore, we show how to reduce the property classification problem into a standard (class) classification problem, which allows reasoners to classify properties using our optimised procedure. We have implemented our algorithms in the OWL HermiT reasoner, and we present the results of a performance evaluation.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2010},
	publisher = {Springer},
	author = {Glimm, Birte and Horrocks, Ian and Motik, Boris and Stoilos, Giorgos},
	editor = {Patel-Schneider, Peter F. and Pan, Yue and Hitzler, Pascal and Mika, Peter and Zhang, Lei and Pan, Jeff Z. and Horrocks, Ian and Glimm, Birte},
	year = {2010},
	pages = {225--240},
}

@article{lawrynowicz_discovery_2018,
	title = {Discovery of emerging design patterns in ontologies using tree mining},
	volume = {9},
	issn = {22104968, 15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-170280},
	doi = {10.3233/SW-170280},
	number = {4},
	urldate = {2022-03-30},
	journal = {Semantic Web},
	author = {Ławrynowicz, Agnieszka and Potoniec, Jedrzej and Robaczyk, Michał and Tudorache, Tania},
	editor = {Hoekstra, Rinke},
	month = jun,
	year = {2018},
	pages = {517--544},
}

@inproceedings{goller_learning_1996,
	title = {Learning task-dependent distributed representations by backpropagation through structure},
	volume = {1},
	doi = {10.1109/ICNN.1996.548916},
	abstract = {While neural networks are very successfully applied to the processing of fixed-length vectors and variable-length sequences, the current state of the art does not allow the efficient processing of structured objects of arbitrary shape (like logical terms, trees or graphs). We present a connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size. The most general structures that can be handled are labeled directed acyclic graphs. The major difference of our approach compared to others is that the structure-representations are exclusively tuned for the intended inference task. Our method is applied to tasks consisting in the classification of logical terms. These range from the detection of a certain subterm to the satisfaction of a specific unification pattern. Compared to previously known approaches we obtained superior results in that domain.},
	booktitle = {Proceedings of {International} {Conference} on {Neural} {Networks} ({ICNN}'96)},
	author = {Goller, C. and Kuchler, A.},
	month = jun,
	year = {1996},
	keywords = {Backpropagation, Computer science, Data structures, Detectors, Information processing, Labeling, Neural networks, Shape, Supervised learning, Tree graphs},
	pages = {347--352 vol.1},
}

@article{paassen_benjamin_mapping_2021,
	title = {Mapping {Python} {Programs} to {Vectors} using {Recursive} {Neural} {Encodings}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International, Open Access},
	url = {https://zenodo.org/record/5634224},
	doi = {10.5281/ZENODO.5634224},
	abstract = {Educational data mining involves the application of data mining techniques to student activity. However, in the context of computer programming, many data mining techniques can not be applied because they require vector-shaped input, whereas computer programs have the form of syntax trees. In this paper, we present ast2vec, a neural network that maps Python syntax trees to vectors and back, thereby enabling about a hundred data mining techniques that were previously not applicable. Ast2vec has been trained on almost half a million programs of novice programmers and is designed to be applied across learning tasks without re-training, meaning that users can apply it without any need for deep learning. We demonstrate the generality of ast2vec in three settings. First, we provide example analyses using ast2vec on a classroom-sized dataset, involving two novel techniques, namely progress-variance projection for visualization and a dynamical systems analysis for prediction. In these examples, we also explain how ast2vec can be utilized for educational decisions. Second, we consider the ability of ast2vec to recover the original syntax tree from its vector representation on the training data and two other large-scale programming datasets. Finally, we evaluate the predictive capability of a linear dynamical system on top of ast2vec, obtaining similar results to techniques that work directly on syntax trees while being much faster (constant- instead of linear-time processing). We hope ast2vec can augment the educational data mining toolkit by making analyses of computer programs easier, richer, and more efficient.},
	language = {en},
	urldate = {2022-03-14},
	author = {{Paassen, Benjamin} and {McBroom, Jessica} and {Jeffries, Bryn} and {Koprinska, Irena} and {Yacef, Kalina}},
	month = oct,
	year = {2021},
	keywords = {computer programs, computer science education, neural networks, program vectors, representation learning, visualization, word embeddings},
}

@inproceedings{flouris_inconsistencies_2006,
	title = {Inconsistencies, negations and changes in ontologies},
	url = {https://abdn.pure.elsevier.com/en/publications/inconsistencies-negations-and-changes-in-ontologies},
	language = {English},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 21st {National} {Conference} on {Artificial} {Intelligence} and the 18th {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {AAAI}-06/{IAAI}-06},
	author = {Flouris, Giorgos and Huang, Zhisheng and Pan, Jeff Z. and Plexousakis, Dimitris and Wache, Holger},
	month = nov,
	year = {2006},
	pages = {1295--1300},
}

@incollection{baader_basic_2003,
	address = {USA},
	title = {Basic description logics},
	isbn = {9780521781763},
	url = {https://www.inf.unibz.it/~franconi/dl/course/dlhb/dlhb-02.pdf},
	abstract = {This chapter provides an introduction to Description Logics as a formal language for representing knowledge and reasoning about it. It first gives a short overview of the ideas underlying Description Logics. Then it introduces syntax and semantics, covering the basic constructors that are used in systems or have been introduced in the literature, and the way these constructors can be used to build knowledge bases. Finally, it defines the typical inference problems, shows how they are interrelated, and describes different approaches for effectively solving these problems. Some of the topics that are only briefly mentioned in this chapter will be treated in more detail in subsequent chapters.},
	urldate = {2022-03-08},
	booktitle = {The description logic handbook: theory, implementation, and applications},
	publisher = {Cambridge University Press},
	author = {Baader, Franz and Nutt, Werner},
	month = jan,
	year = {2003},
	pages = {43--95},
}

@inproceedings{tsarkov_fact_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{FaCT}++ {Description} {Logic} {Reasoner}: {System} {Description}},
	isbn = {9783540371885},
	shorttitle = {{FaCT}++ {Description} {Logic} {Reasoner}},
	doi = {10.1007/11814771_26},
	abstract = {This is a system description of the Description Logic reasoner FaCT++. The reasoner implements a tableaux decision procedure for the well known {\textbackslash}({\textbackslash}mathcal\{SHOIQ\}{\textbackslash}) description logic, with additional support for datatypes, including strings and integers. The system employs a wide range of performance enhancing optimisations, including both standard techniques (such as absorption and model merging) and newly developed ones (such as ordering heuristics and taxonomic classification). FaCT++ can, via the standard DIG interface, be used to provide reasoning services for ontology engineering tools supporting the OWL DL ontology language.},
	language = {en},
	booktitle = {Automated {Reasoning}},
	publisher = {Springer},
	author = {Tsarkov, Dmitry and Horrocks, Ian},
	editor = {Furbach, Ulrich and Shankar, Natarajan},
	year = {2006},
	pages = {292--297},
}

@article{huang_comparison_2008,
	title = {Comparison of {Ontology} {Reasoners}: {Racer}, {Pellet}, {Fact}++},
	volume = {2008},
	shorttitle = {Comparison of {Ontology} {Reasoners}},
	url = {https://ui.adsabs.harvard.edu/abs/2008AGUFMIN13A1068H},
	abstract = {In this paper, we examine some key aspects of three of the most popular and effective Semantic reasoning engines that have been developed: Pellet, RACER, and Fact++. While these reasonably advanced reasoners share some notable similarities, it is ultimately the creativity and unique nature of these reasoning engines that have resulted in the successes of each of these reasoners. Of the numerous dissimilarities, the most obvious example might be that while Pellet is written in Java, RACER employs the Lisp programming language and Fact++ was developed using C++. From this and many other distinctions in the system architecture, we can understand the benefits of each reasoner and potentially discover certain properties that may contribute to development of an optimal reasoner in the future. The objective of this paper is to establish a solid comparison of the reasoning engines based on their system architectures, features, and overall performances in real world application. In the end, we expect to produce a valid conclusion about the advantages and problems in each reasoner. While there may not be a decisive first place among the three reasoners, the evaluation will also provide some answers as to which of these current reasoning tools will be most effective in common, practical situations.},
	urldate = {2022-03-07},
	author = {Huang, T. and Li, W. and Yang, C.},
	month = dec,
	year = {2008},
	note = {ADS Bibcode: 2008AGUFMIN13A1068H},
	keywords = {0545 Modeling (4255), 1800 HYDROLOGY, 1819 Geographic Information Systems (GIS), 6339 System design},
	pages = {IN13A--1068},
}

@inproceedings{eberhart_pseudo-random_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pseudo-{Random} {ALC} {Syntax} {Generation}},
	isbn = {9783319981925},
	doi = {10.1007/978-3-319-98192-5_4},
	abstract = {We discuss a tool capable of rapidly generating pseudo-random syntactically valid ALC expression trees [1]. The program is meant to allow a researcher to create large sets of independently valid expressions with a minimum of personal bias for experimentation.},
	language = {en},
	booktitle = {The {Semantic} {Web}: {ESWC} 2018 {Satellite} {Events}},
	publisher = {Springer International Publishing},
	author = {Eberhart, Aaron and Cheatham, Michelle and Hitzler, Pascal},
	editor = {Gangemi, Aldo and Gentile, Anna Lisa and Nuzzolese, Andrea Giovanni and Rudolph, Sebastian and Maleshkova, Maria and Paulheim, Heiko and Pan, Jeff Z and Alam, Mehwish},
	year = {2018},
	pages = {19--22},
}

@article{ebrahimi_towards_2021,
	title = {Towards bridging the neuro-symbolic gap: deep deductive reasoners},
	volume = {51},
	issn = {1573-7497},
	shorttitle = {Towards bridging the neuro-symbolic gap},
	url = {https://doi.org/10.1007/s10489-020-02165-6},
	doi = {10.1007/s10489-020-02165-6},
	abstract = {Symbolic knowledge representation and reasoning and deep learning are fundamentally different approaches to artificial intelligence with complementary capabilities. The former are transparent and data-efficient, but they are sensitive to noise and cannot be applied to non-symbolic domains where the data is ambiguous. The latter can learn complex tasks from examples, are robust to noise, but are black boxes; require large amounts of –not necessarily easily obtained– data, and are slow to learn and prone to adversarial examples. Either paradigm excels at certain types of problems where the other paradigm performs poorly. In order to develop stronger AI systems, integrated neuro-symbolic systems that combine artificial neural networks and symbolic reasoning are being sought. In this context, one of the fundamental open problems is how to perform logic-based deductive reasoning over knowledge bases by means of trainable artificial neural networks. This paper provides a brief summary of the authors’ recent efforts to bridge the neural and symbolic divide in the context of deep deductive reasoners. Throughout the paper we will discuss strengths and limitations of models in term of accuracy, scalability, transferability, generalizabiliy, speed, and interpretability, and finally, will talk about possible modifications to enhance desirable capabilities. More specifically, in terms of architectures, we are looking at Memory-augmented networks, Logic Tensor Networks, and compositions of LSTM models to explore their capabilities and limitations in conducting deductive reasoning. We are applying these models on Resource Description Framework (RDF), first-order logic, and the description logic \${\textbackslash}mathcal \{E\}\{{\textbackslash}mathscr\{L\}\}{\textasciicircum}\{+\}\$respectively.},
	language = {en},
	number = {9},
	urldate = {2021-12-09},
	journal = {Applied Intelligence},
	author = {Ebrahimi, Monireh and Eberhart, Aaron and Bianchi, Federico and Hitzler, Pascal},
	month = sep,
	year = {2021},
	pages = {6326--6348},
}

@article{guo_lubm_2005,
	series = {Selcted {Papers} from the {International} {Semantic} {Web} {Conference}, 2004},
	title = {{LUBM}: {A} benchmark for {OWL} knowledge base systems},
	volume = {3},
	issn = {1570-8268},
	shorttitle = {{LUBM}},
	url = {https://www.sciencedirect.com/science/article/pii/S1570826805000132},
	doi = {10.1016/j.websem.2005.06.005},
	abstract = {We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memory-based systems and two systems with persistent storage.},
	language = {en},
	number = {2},
	urldate = {2022-02-02},
	journal = {Journal of Web Semantics},
	author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
	month = oct,
	year = {2005},
	keywords = {Evaluation, Knowledge base system, Lehigh University Benchmark, Semantic Web},
	pages = {158--182},
}

@article{badreddine_logic_2021,
	title = {Logic {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2012.13635},
	abstract = {Artificial Intelligence agents are required to learn from their surroundings and to reason about the knowledge that has been learned in order to make decisions. While state-of-the-art learning from data typically uses sub-symbolic distributed representations, reasoning is normally useful at a higher level of abstraction with the use of a first-order logic language for knowledge representation. As a result, attempts at combining symbolic AI and neural computation into neural-symbolic systems have been on the increase. In this paper, we present Logic Tensor Networks (LTN), a neurosymbolic formalism and computational model that supports learning and reasoning through the introduction of a many-valued, end-to-end differentiable first-order logic called Real Logic as a representation language for deep learning. We show that LTN provides a uniform language for the specification and the computation of several AI tasks such as data clustering, multi-label classification, relational learning, query answering, semi-supervised learning, regression and embedding learning. We implement and illustrate each of the above tasks with a number of simple explanatory examples using TensorFlow 2. Keywords: Neurosymbolic AI, Deep Learning and Reasoning, Many-valued Logic.},
	urldate = {2021-12-13},
	journal = {arXiv:2012.13635 [cs]},
	author = {Badreddine, Samy and Garcez, Artur d'Avila and Serafini, Luciano and Spranger, Michael},
	month = jan,
	year = {2021},
	note = {arXiv: 2012.13635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
}

@inproceedings{ongenae_otagen_2008,
	title = {{OTAGen}: {A} {Tunable} {Ontology} {Generator} for {Benchmarking} {Ontology}-{Based} {Agent} {Collaboration}},
	shorttitle = {{OTAGen}},
	doi = {10.1109/COMPSAC.2008.134},
	abstract = {On the one hand, agent-based software platforms are commonly used these days, while on the other hand Semantic Web technologies are also maturing. It is obvious that the combination of these two technologies can bring added value through the creation of Semantic Agent-based frameworks. However, it is also known that these Semantic Web technologies, and the reasoning on ontologies in particular, can rapidly become resource intensive. In order to get a clear view on this problem, we have developed OTAGen, a highly tunable tool to generate customized ontologies and corresponding queries. The generated ontologies can then be used to evaluate at design-time the performance of the Semantic Agent-based platform as a function of the number of ontologies, users and queries.},
	booktitle = {2008 32nd {Annual} {IEEE} {International} {Computer} {Software} and {Applications} {Conference}},
	author = {Ongenae, F. and Verstichel, S. and De Turck, F. and Dhaene, T. and Dhoedt, B. and Demeester, P.},
	month = jul,
	year = {2008},
	note = {ISSN: 0730-3157},
	keywords = {Agent, Benchmark testing, Cognition, Collaboration, Complexity theory, OTAGen, Ontologies, Ontology, Performance, Reasoning, Semantic Agent, Semantic Web, Semantic web, Web services},
	pages = {529--530},
}

@article{selsam_learning_2019,
	title = {Learning a {SAT} {Solver} from {Single}-{Bit} {Supervision}},
	url = {http://arxiv.org/abs/1802.03685},
	abstract = {We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.},
	urldate = {2021-12-09},
	journal = {arXiv:1802.03685 [cs]},
	author = {Selsam, Daniel and Lamm, Matthew and Bünz, Benedikt and Liang, Percy and de Moura, Leonardo and Dill, David L.},
	month = mar,
	year = {2019},
	note = {arXiv: 1802.03685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
}

@inproceedings{shearer_hermit_2008,
	title = {{HermiT}: {A} {Highly}-{Efficient} {OWL} {Reasoner}},
	shorttitle = {{HermiT}},
	abstract = {Tests show that HermiT is usually much faster than other reasoners when classifying complex ontologies, and it is already able to classify a number of ontologies which no other reasoner has been able to handle. HermiT is a new OWL reasoner based on a novel “hypertableau” calculus. The new calculus addresses performance problems due to nondeterminism and model size—the primary sources of complexity in state-of-the-art OWL reasoners. The latter is particularly important in practice, and it is achieved in HermiT with an improved blocking strategy and and an optimization that tries to reuse existing individuals rather than generating new ones. HermiT also incorporates a number of other novel optimizations, such as a more efficient approach to handling nominals, and various techniques for optimizing ontology classification. Our tests show that HermiT is usually much faster than other reasoners when classifying complex ontologies, and it is already able to classify a number of ontologies which no other reasoner has been able to handle.},
	booktitle = {{OWLED}},
	author = {Shearer, Rob and Motik, B. and Horrocks, I.},
	year = {2008},
}

@misc{noauthor_distributional_2021,
	title = {Distributional semantics},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Distributional_semantics&oldid=1061390267},
	abstract = {Distributional semantics is a research area that develops and studies theories and methods for quantifying and categorizing semantic similarities between linguistic items based on their distributional properties in large samples of language data. The basic idea of distributional semantics can be summed up in the so-called Distributional hypothesis: linguistic items with similar distributions have similar meanings.},
	language = {en},
	urldate = {2022-01-29},
	journal = {Wikipedia},
	month = dec,
	year = {2021},
	note = {Page Version ID: 1061390267},
}

@article{benammar_riyadh_consistent_2014,
	title = {Consistent {Random} {Ontology} generation},
	url = {http://rgdoi.net/10.13140/2.1.4654.4167},
	doi = {10.13140/2.1.4654.4167},
	urldate = {2022-02-02},
	author = {{Benammar Riyadh} and {Chevalier Jules}},
	year = {2014},
}

@inproceedings{ma_towards_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards a {Complete} {OWL} {Ontology} {Benchmark}},
	isbn = {9783540345459},
	doi = {10.1007/11762256_12},
	abstract = {Aiming to build a complete benchmark for better evaluation of existing ontology systems, we extend the well-known Lehigh University Benchmark in terms of inference and scalability testing. The extended benchmark, named University Ontology Benchmark (UOBM), includes both OWL Lite and OWL DL ontologies covering a complete set of OWL Lite and DL constructs, respectively. We also add necessary properties to construct effective instance links and improve instance generation methods to make the scalability testing more convincing. Several well-known ontology systems are evaluated on the extended benchmark and detailed discussions on both existing ontology systems and future benchmark development are presented.},
	language = {en},
	booktitle = {The {Semantic} {Web}: {Research} and {Applications}},
	publisher = {Springer},
	author = {Ma, Li and Yang, Yang and Qiu, Zhaoming and Xie, Guotong and Pan, Yue and Liu, Shengping},
	editor = {Sure, York and Domingue, John},
	year = {2006},
	keywords = {Description Logic , Ontology System , Query Response Time , Query Time , Real Ontology },
	pages = {125--139},
}

@incollection{porello_two_2018,
	title = {Two {Approaches} to {Ontology} {Aggregation} {Based} on {Axiom} {Weakening}},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {\textbackslash}texttt{\textbackslash}{char92textbraceleftIJCAI}{\textbackslash}texttt{\textbackslash}char92textbraceright 2018, {July} 13-19, 2018, {Stockholm}, {Sweden}.},
	author = {Porello, Daniele and Troquard, Nicolaas and Kutz, Oliver and Penaloza, Rafael and Confalonieri, Roberto and Galliani, Pietro},
	year = {2018},
	pages = {1942--1948},
}
