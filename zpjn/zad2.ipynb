{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ocena: 93%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "W szczególności zwróć uwagę, że usupełniłeś wszystkie miejsca `YOUR CODE HERE`, `WPISZ TWÓJ KOD TUTAJ`, \"YOUR ANSWER HERE\" lub \"WPISZ TWOJĄ ODPOWIEDŹ TUTAJ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaawansowane Przetwarzanie Języka Naturalnego\n",
    "# Laboratorium 2\n",
    "\n",
    "Pobierz zbiór danych Amazon \"Musical Instruments\" z [tej](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz) strony internetowej, a następnie wczytaj go poniższym kodem. Zwróć uwagę na wymaganą lokalizację pliku, tj. dwa katalogi wyżej - wynika to ze struktury plików w sprawdzarce, przepraszam za niedogodność.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.794663Z",
     "start_time": "2021-12-01T20:15:31.401431Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    " \n",
    "x_text = []\n",
    "y = []\n",
    "with open('../../Musical_Instruments_5.json') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        x_text.append(data['reviewText'].lower().strip())\n",
    "        y.append(int(data['overall']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 - przygotowanie danych\n",
    "W załadowanych listach `x_text` oraz `y` znajdują się odpowiednio teksty kolejnych opinii oraz oznaczenia klas. Klasą w tym wypadku jest liczba gwiazdek (ocena) produktu towarzysząca opinii. Zadanie klasyfikacji polega na przewidzeniu oceny na podstawie opinii pozostawionej w portalu.\n",
    "\n",
    "Aby zmniejszyć wymagania obliczeniowe do dalszych eksperymentów, ograniczymy zbiór danych jedynie do pierwszego tysiąca opinii.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.798404Z",
     "start_time": "2021-12-01T20:15:32.795687Z"
    }
   },
   "outputs": [],
   "source": [
    "x_text = x_text[:1000]\n",
    "y = y[:1000]\n",
    "train_end_idx=int(0.9 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedną z użytecznych operacji przygotowania tekstu do konstrukcji klasyfikatora jest zastąpienie poszczególnych tokenów ich indeksami. Chociaż w praktyce ten proces często następuje dopiero po szeregu etapów przetwarzania tekstu takich jak tokenizacja, lematyzacja czy stemming - w tym ćwiczeniu wyodrębnimy tokeny rozdzielając tekst znakiem spacji.\n",
    "\n",
    "Klasyfikator powinien obsługiwać także słowa, które nie występowały w zbiorze uczącym. Podstawową techniką obsługi takich słów jest wprowadzenie specjalnego tokenu UNK, obsługującego nieznane słowa. W tym celu usuwa się ze zbioru danych pewną liczbę najrzadszych słów i zastępuje się je tokenami UNK.\n",
    "\n",
    "Zbuduj słownik `w2i` mapujący tokeny na kolejne indeksy tj. liczby naturalne. Pomiń tokeny występujące w zbiorze uczącym 5 lub mniej razy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.827514Z",
     "start_time": "2021-12-01T20:15:32.799294Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f866dd9918a27f9ba2f31ef62a061fa1",
     "grade": false,
     "grade_id": "cell-87edb8b6e5279a0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<unk>\"] #Przypisz indeks tokenowi UNK\n",
    "\n",
    "tokens = []\n",
    "for x in x_text:\n",
    "    tokens += x.split(' ')\n",
    "counts = Counter(tokens)\n",
    "for w, n in counts.items():\n",
    "    if n > 5:\n",
    "        w2i[w]\n",
    "    \n",
    "n_words = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b1000affcd1be506cad1e3a22a63cf1",
     "grade": true,
     "grade_id": "cell-59701c5db1041735",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zbudowaniu słownika `w2i`, przekonwertujmy nasz zbiór danych z listy słów na listę indeksów słów. Od razu podzielimy zbiór na część uczącą i część testową, a także przekonwertujemy klasy na indeksy klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.862455Z",
     "start_time": "2021-12-01T20:15:32.829918Z"
    }
   },
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: UNK, w2i) # Domyślną wartością słownika jest UNK, \n",
    "         #chociaż w2i będzie zawierał wpisy do wszystkich słów to nowym tokenom będzie przypisywał indeks UNK\n",
    "class2i = defaultdict(lambda: len(class2i))\n",
    "        # mapuj klasy na indeksy klas\n",
    "    \n",
    "def read_dataset(start_idx,end_idx):\n",
    "    for i, text in enumerate(x_text[start_idx:end_idx]):\n",
    "        yield ([w2i[x] for x in text.split(\" \")], class2i[y[i]])\n",
    "        \n",
    "train = list(read_dataset(0, train_end_idx))\n",
    "dev = list(read_dataset(train_end_idx, len(y)))\n",
    "n_class = len(class2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.879065Z",
     "start_time": "2021-12-01T20:15:32.863276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389 5\n"
     ]
    }
   ],
   "source": [
    "print(n_words, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 - pierwszy model klasyfikacji tekstu w PyTorch\n",
    "Podstawową strukturą danych w PyTorch jest tensor, na którym możesz wykonywać analogiczne operacje jak na macierzach `numpy`. Podstawową metodą stworzenia tensora jest wywołanie konstruktora `torch.tensor` na liście liczb. Istnieją także inne konstruktory tensorów, analogiczne do `numpy`. Można też na nich operować za pomocą standardowych operatorów, indeksowania, i odpowiedników innych funkcji znanych z `numpy`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.911797Z",
     "start_time": "2021-12-01T20:15:32.879902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[0.1402, 0.6186, 0.0339],\n",
      "        [0.4701, 0.1643, 0.7659],\n",
      "        [0.1705, 0.4139, 0.1269]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1,2,3]))\n",
    "print(torch.rand( (3,3) ))\n",
    "print(torch.ones( (3,3) ))\n",
    "print(2 * torch.ones( (3,3) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dlaczego więc korzystamy z PyTorch, a nie z biblioteki `numpy` skoro tensory wydają się mieć analogiczną funkcjonalność do poznanych uprzednio macierzy? Powodów jest oczywiście wiele, m.in. możliwość przeniesienia obliczeń na kartę graficzną (technologia CUDA), ale z punktu widzenia naszego ćwiczenia kluczowa jest funkcjonalność automatycznego liczenia gradientów. W przypadku konstrukcji sieci neuronowej czy modelu liniowego, w PyTorch nie jest konieczne samodzielne wyprowadzanie i implementowanie gradientu, gdyż biblioteka zrobi to za nas automatycznie.\n",
    "\n",
    "Wyznaczanie gradientów odbywa się za pomocą algorytmu wstecznej propagacji, który ma dwie fazy: *forward* i *backward*. Faza *forward* polega na policzeniu wyniku funkcji, a faza *backward* wyznacza gradienty wszystkich jej parametrów.\n",
    "\n",
    "W celu poznania tej funkcjonalności policzymy pochodne cząstkowe prostej funkcji kwadratowej:\n",
    "$$result = x_1^2 + x_2^2+ x_3^2$$\n",
    "której pochodne cząstkowe mają postać $2x_i$.\n",
    "\n",
    "Rozpocznijmy implementacje tej funkcji od stworzenia 3-elementowego wektora zmiennych `x`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.925257Z",
     "start_time": "2021-12-01T20:15:32.912836Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1.,2.,3.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak pewnie zauważyłeś, w konstruktorze użyliśmy dodatkowego parametru `requires_grad`. Domyślnie wykonanie operacji na dowolnym tensorze nie traktuje się jako części fazy `forward`, gdyż nie do wszystkich tensorów użytych w kodzie będziemy potrzebować wartości pochodnych. Aby zasygnalizować, że dla danej zmiennej konieczne jest zapisywanie informacji o wykonywanych na niej operacjach, należy ustawić wartość jej parametru `requires_grad` na `True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.946691Z",
     "start_time": "2021-12-01T20:15:32.926058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy do policzenia wartości wyżej zdefiniowanej funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.961957Z",
     "start_time": "2021-12-01T20:15:32.947549Z"
    }
   },
   "outputs": [],
   "source": [
    "result = (x**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensory posiadają parametr `.grad`, który przechowuje informacje o wyznaczonym gradiencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:32.985259Z",
     "start_time": "2021-12-01T20:15:32.963565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tej chwili, pomimo obliczenia wartości zmiennej `result`, wartość gradientu nie jest policzona, gdyż nie poinformowaliśmy biblioteki o zakończeniu fazy `forward` i konieczności wykonania fazy `backward`. Możemy to zrobić poprzez wykonanie funkcji `backward()` na obliczonej wartości funkcji (funkcję tę można wywołać tylko na skalarnym wyniku!).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.006152Z",
     "start_time": "2021-12-01T20:15:32.986193Z"
    }
   },
   "outputs": [],
   "source": [
    "result.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.034724Z",
     "start_time": "2021-12-01T20:15:33.006935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że wektor `x.grad`, zgodnie z naszymi oczekiwaniami, zawiera wartości pochodnej cząstkowej tj. `2x`. Spróbujmy jeszcze raz, licząc pochodną po logarytmie z `result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.082359Z",
     "start_time": "2021-12-01T20:15:33.036082Z"
    }
   },
   "outputs": [],
   "source": [
    "result2 = torch.log(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.094104Z",
     "start_time": "2021-12-01T20:15:33.083649Z"
    }
   },
   "outputs": [],
   "source": [
    "#result2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niestety operacja się nie powiodła. Przed wykonaniem kolejnej fazy *backward* należy - upraszczając - wykonać fazę *forward*. Nasze poprzednie operacje konstruowały fazę *forward* od parametrów `x` aż do zmiennej z wynikiem, jednakże przy wykonaniu fazy *backward* została zwolniona pamięć przechowująca informacje o kolejno wykonywanych operacjach na tych zmiennych (graf obliczeń). Kolejna operacja została wykonana bezpośrednio na tensorze `result`, konstruując fazę *forward* od `result` do `result2`, jednak zabrakło grafu obliczeń od parametrów `x`.\n",
    "\n",
    "Uruchomienie poniższego kodu, z operacjami rozpoczynającymi się od `x`, zakończy się obliczeniem gradientu z sukcesem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.116656Z",
     "start_time": "2021-12-01T20:15:33.095348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1429, 4.2857, 6.4286])\n"
     ]
    }
   ],
   "source": [
    "result = (x**2).sum()\n",
    "result2 = torch.log(result)\n",
    "result2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność uzyskanego wyniku. Zmienna $result= 1^2+2^2+3^2=14$, a pochodna z logarytmu naturalnego to $\\frac{1}{x}$. W związku z tym:\n",
    "$$\\frac{\\partial }{\\partial x_1} \\log result = \\frac{1}{result} \\cdot \\frac{\\partial }{\\partial x_1} result = \\frac{1}{result} 2x_1 $$\n",
    "Przy naszych wartościach $x$ równa się to $\\frac{1}{14}\\cdot 2 = 0,1428$. Łatwo zauważyć, że wynik znajdujący się w tensorze `x.grad` jest błędny, a konkretnie za duży o 2 jednostki.\n",
    "\n",
    "Stało się tak dlatego, że gradient z kolejnych faz `backward` jest akumulowany w parametrze `.grad` (poprzednia wartość policzonej pochodnej cząstkowej wynosiła właśnie 2). Takie zachowanie biblioteki może być bardzo użyteczne w sytuacji gdy chcemy w zmiennej zagregować gradienty funkcji celu liczonych na kolejno przetwarzanych instancjach lub przy treningu modelu z wieloma funkcjami celu; tutaj jednak doprowadziło to do błędnego wyniku. Z tego powodu bardzo ważne jest pamiętanie o wyzerowaniu wartości gradientów przed przystąpieniem do kolejnych obliczeń.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.139875Z",
     "start_time": "2021-12-01T20:15:33.117565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.156045Z",
     "start_time": "2021-12-01T20:15:33.142012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1429, 0.2857, 0.4286])\n"
     ]
    }
   ],
   "source": [
    "result = (x**2).sum()\n",
    "result2 = torch.log(result)\n",
    "result2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę na konwencję biblioteki PyTorch - jeśli nazwa funkcji zakończona jest podkreślnikiem to taka operacja jest wykonywana `in-place`. (np. `x.add(5)` - `x` nadal ma stałą wartość, `x.add_(5)` wartość `x` zwiększono o 5).\n",
    "\n",
    "Zaimplementujmy podstawowy algorytm uczący w PyTorch. Będzie to prosta sieć neuronowa składająca się z:\n",
    "- macierzy zanurzeń $C$, przetwarzającej indeksy słów na odpowiednie reprezentacje wektorowe, \n",
    "- operacji uśredniania tych zanurzeń do jednego zanurzenia (average pooling over time) \n",
    "- oraz jednej warstwy liniowej (softmax) zwracającej wynik.\n",
    "\n",
    "Algorytmem uczącym będzie SGD optymalizujące entropię krzyżową, czyli dla kolejnych instancji uczących będziemy wykonywać:\n",
    "$$parametry = parametry - \\eta \\nabla f\\_celu$$\n",
    "Implementacja ta będzie wyjątkowo prosta, gdyż gradient funkcji celu ($\\nabla f\\_celu$) zostanie obliczony automatycznie przez PyTorch. Ponadto entropia krzyżowa jest już zaimplementowana w PyTorch `F.cross_entropy(logits, target)`. Zwróć uwagę, że argumentem tej funkcji są wartości logitów (nie trzeba implementować funkcji softmax przetwarzającej wartości logitów na prawdopodobieństwa).\n",
    "\n",
    "**UWAGA** W implementacji nie należy używać gotowych implementacji SGD czy warstw sieci neuronowych w PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem w implementacji będzie zaimplementowanie samego modelu. Należy zainicjalizować macierz $C$ przechowującą w wierszach zanurzenia dla kolejnych słów (liczba słów to `n_words`, wymiarowość zanurzenia określ na 20) oraz macierz $W$ przechowującą parametry warstwy liniowej, zwracającej wartości logitów dla każdej z klas (liczba klas to `n_class`). Macierz $W$ w dodatkowej kolumnie powinna też przechowywać wartości wyrazów wolnych (bias). Wartości zainicjalizuj losowo `torch.rand`. Pamiętaj, że dla tych macierzy będziesz potrzebował wyznaczyć potem wartości gradientów.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.183104Z",
     "start_time": "2021-12-01T20:15:33.156870Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134cf7cd8660133b178caff1ca2fcdfc",
     "grade": false,
     "grade_id": "cell-9ba79ce47d0c6a5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 20\n",
    "C = torch.rand((n_words, EMBEDDING_SIZE), requires_grad=True)\n",
    "W = torch.rand((n_class, EMBEDDING_SIZE+1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "084514b8fbf6ffa4f784b5b398bd3624",
     "grade": true,
     "grade_id": "cell-b3436825d33d5941",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj funkcję `simple_model`, której argumentem będzie instancja testowa (jest to więc lista indeksów słów występujących w tekście), a na której wyjściu będzie wektor `n_class`-elementowy zawierający obliczone wartości logitów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:33.203413Z",
     "start_time": "2021-12-01T20:15:33.184434Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "280f8962c6b7f20c718a51b6ecc294a4",
     "grade": false,
     "grade_id": "cell-e2668a9abacbc78a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_model(x):\n",
    "    doc_embedding = C[x].mean(0)\n",
    "    doc_with_bias = torch.cat([torch.ones(1),doc_embedding]) # Skonkatenowanie 1 z uzyskaną reprezentacją (bias)\n",
    "    return W @ doc_with_bias # Obliczenie wartości logitów (tj. warstwa liniowa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj algorytm SGD w poniższej pętli. Pętla ta iteruje po zbiorze uczącym oraz dla każdej instancji oblicza wartość funkcji celu. Twoje zadania:\n",
    "- Policz gradienty (faza *backward*)\n",
    "- Zaimplementuj aktualizacje $W$ i $C$ wg. wzoru na SGD. Operacje modyfikujące $W$ i $C$ musisz wykonać w środku klauzuli `with torch.no_grad():`, aby nie śledzić z tych operacji gradientów.\n",
    "- Pamiętaj o wyczyszczeniu gradientów (zarówno w $W$ jak i $C$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:34.276407Z",
     "start_time": "2021-12-01T20:15:33.204919Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cfb391b5dfe1e8de9ca493a895d47eb",
     "grade": false,
     "grade_id": "cell-aeedd0f5183bd781",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=1.0852\n",
      "iter 1: avg. train loss=0.9739\n",
      "iter 2: avg. train loss=0.9388\n",
      "iter 3: avg. train loss=0.9146\n",
      "iter 4: avg. train loss=0.8692\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = simple_model(words)\n",
    "        loss = F.cross_entropy(pred.view(1,-1), torch.tensor([tag]))\n",
    "        # Loss zawiera wartość funkcji celu dla przykładu, wykonaj backpropagation\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            train_loss += loss\n",
    "            W -= eta * W.grad\n",
    "            C -= eta * C.grad\n",
    "            W.grad.zero_()\n",
    "            C.grad.zero_()\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Dlaczego w bibliotekach do głębokiego uczenia maszynowego, takich jak PyTorch, implementuje się funkcje entropii krzyżowej tak, aby przyjmowała na wejście wartości logitów zamiast prawdopodobieństw z softmax?\n",
    "- Na wykładzie pokazywaliśmy warstwę zanurzeń jako warstwę mnożącą macierz $C$ przez wektor \"1 z n\", można ją jednak także zaimplementować jako operację odczytu odpowiedniego wiersza z macierzy. Jakie są wady i zalety obu tych sposobów implementacji?\n",
    "\n",
    "Odpowiedź na pierwszą kropkę umieść poniżej.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5158398dc380a8a01088a617375d8f4c",
     "grade": true,
     "grade_id": "cell-c685493684bda3e3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Odp: Obliczenie entropii krzyżowej wzraz z softmaxem jest szybsze niż zrobienie tego oddzielnie, bo wzór się upraszcza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 - wykorzystanie nn.Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch jako biblioteka do głębokiego uczenia maszynowego oferuje nam kilka udogodnień w implementowaniu modeli uczących się, aby jeszcze bardziej uprościć ich implementację. Większość z tych udogodnień związanych z jest z reprezentowaniem modeli uczących się jako obiektów dziedziczących po `torch.nn.Module`. W obiekcie takim powinniśmy zaimplementować co najmniej konstruktor, inicjalizujący parametry modelu ($W$ i $C$), oraz funkcję `forward` obliczającą wynik modelu (we wcześniejszym zadaniu nazywaliśmy ją `simple_model`). Ponadto moduł `torch.nn` oferuje gotowe implementacje zarówno warstwy liniowej jak i warstwy zanurzeń.\n",
    "\n",
    "Przeanalizuj poniższą implementację modelu z poprzedniego zadania.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:34.282315Z",
     "start_time": "2021-12-01T20:15:34.278298Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, n_class):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(n_words, emb_size)\n",
    "        self.linear = torch.nn.Linear(in_features=emb_size, out_features=n_class, bias=True)\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)                 \n",
    "        h = emb.mean(dim=0)                         \n",
    "        h = torch.reshape(h, (1,-1))\n",
    "        out = self.linear(h)              \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz tego, że uzyskaliśmy elegancki obiekt reprezentujący nasz model, nie wydaje się by powyższa implementacja była krótsza czy prostsza od tej, którą uzyskaliśmy w poprzednim zadaniu bez dobrodziejstw `nn.Module`. Co zatem zyskaliśmy?\n",
    "\n",
    "Przy implementacji modeli z dużą liczbą warstw, szczególnie uciążliwe byłoby implementowanie kolejnych linijek kodu zerujących gradienty wszystkich macierzy wag, oraz wykonywanie na nich kroków algorytmu SGD. W naszej implementacji każda macierz parametrów to dwie linijki kodu! Jednak modele dziedziczące po `torch.nn.Module` i stworzone poprzez dedykowane warstwy neuronowe posiadają gotową funkcję `parameters()` zwracającą kolejne macierze parametrów modelu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:34.309909Z",
     "start_time": "2021-12-01T20:15:34.283331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.1812,  0.2495,  0.0110,  ...,  0.1097, -0.0516,  0.1309],\n",
      "        [ 0.0152, -0.0732,  0.1146,  ..., -0.1828, -0.1027, -0.2486],\n",
      "        [-0.1966, -0.1615, -0.2158,  ...,  0.0362,  0.0630, -0.0280],\n",
      "        ...,\n",
      "        [ 0.0526,  0.1463, -0.1733,  ..., -0.0457, -0.0253,  0.1771],\n",
      "        [ 0.1621, -0.0074, -0.0734,  ..., -0.0666,  0.2035,  0.0155],\n",
      "        [-0.1520,  0.0056,  0.0598,  ...,  0.0852,  0.1737, -0.0841]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0439, -0.3488,  0.0977,  0.4673, -0.1027, -0.1554, -0.2848, -0.2669,\n",
      "         -0.2168, -0.4868,  0.1690, -0.4131,  0.3919,  0.4054, -0.3726, -0.0270,\n",
      "         -0.2248,  0.0100, -0.2377, -0.1032],\n",
      "        [-0.3339,  0.2454,  0.1740,  0.0157,  0.1613,  0.1484,  0.2063,  0.1026,\n",
      "          0.0415, -0.4485, -0.1324,  0.1367,  0.1363, -0.4293,  0.4496,  0.0619,\n",
      "         -0.0677,  0.4465,  0.3372, -0.4436],\n",
      "        [ 0.2602,  0.4350, -0.3388, -0.1982, -0.0112,  0.2825,  0.2153,  0.1751,\n",
      "         -0.0163, -0.4137,  0.1119,  0.4048,  0.0228, -0.2470,  0.3895, -0.1965,\n",
      "         -0.1758, -0.0425, -0.0227,  0.3507],\n",
      "        [ 0.1391, -0.2461,  0.2655, -0.1572, -0.4661,  0.2851, -0.3041, -0.4624,\n",
      "         -0.4358,  0.2627,  0.4353, -0.2173, -0.3665, -0.0145, -0.2660,  0.0858,\n",
      "          0.3142, -0.1569, -0.2164,  0.0472],\n",
      "        [-0.2506, -0.4507,  0.0025,  0.0519,  0.1842, -0.0594, -0.2507,  0.1238,\n",
      "         -0.2623, -0.4700, -0.1143, -0.1598, -0.3579, -0.0085,  0.4734,  0.3010,\n",
      "         -0.2674,  0.3475, -0.0034,  0.3473]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2021,  0.1487, -0.0457, -0.0560,  0.1787], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(n_words, EMBEDDING_SIZE, n_class)\n",
    "print([i for i in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to niezwykle wygodne, bo implementacja algorytmu SGD może przeiterować po tej liście parametrów i dla każdej z nich wykonać aktualizację ich wartości. Fakt, że taka lista jest tworzona automatycznie pozbawia nas ryzyka, że zwyczajnie o którejś macierzy parametrów czy wektorze wyrazów wolnych najzwyczajniej zapomnimy. \n",
    "\n",
    "Podobnie można zaimplementować pętlę zerującą gradienty wszystkich parametrów. Modele oferują nawet gotową taką funkcję `model.zero_grad()`, która iteruje po parametrach zerując ich gradienty. \n",
    "\n",
    "Zmodyfikuj implementację SGD z poprzedniego zadania, tak aby wykorzystywała `zero_grad()` i `parameters()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:35.692521Z",
     "start_time": "2021-12-01T20:15:34.310892Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5f2f3a6cb7d7b2d2e34e63a2b54004b",
     "grade": false,
     "grade_id": "cell-87d2dfc0c801725a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.9679\n",
      "iter 1: avg. train loss=0.9553\n",
      "iter 2: avg. train loss=0.9206\n",
      "iter 3: avg. train loss=0.8936\n",
      "iter 4: avg. train loss=0.8661\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.view(1,-1), torch.tensor([tag]))\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= eta * param.grad\n",
    "            train_loss += loss\n",
    "            model.zero_grad()\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo moduł `torch.nn` oferuje także od razu zaimplementowane optymalizatory, w tym SGD. W konstruktorze optymalizatora należy podać listę optymalizowanych przez niego parametrów, a następnie wywołać na nim procedurę `step()` wykonującą krok algorytmu optymalizacyjnego tj. aktualizację wartości zmiennych przy użyciu gradientu. W tej sytuacji nie musisz się martwić o umieszczanie kodu zmieniającego parametry w `with torch.no_grad()` - optymalizator sam to zrobi! Optymalizator również oferuje funkcję `zero_grad()`, zerującą gradienty zmiennych wskazanych do optymalizacji.\n",
    "\n",
    "Zmodyfikuj kod z poprzedniego zadania, tak aby wykorzystywał optymalizator SGD zaimplementowany w `torch.optim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:36.983156Z",
     "start_time": "2021-12-01T20:15:35.695287Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92b92a0626a6c533b3b61aa2e6c18ddd",
     "grade": false,
     "grade_id": "cell-5588a40beb11d256",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.8394\n",
      "iter 1: avg. train loss=0.7984\n",
      "iter 2: avg. train loss=0.7830\n",
      "iter 3: avg. train loss=0.7602\n",
      "iter 4: avg. train loss=0.7130\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.view(1,-1), torch.tensor([tag]))\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Przeanalizuj dokładnie powyższy kod i przechodząc linia po linii, wyjaśnij co one robią z punktu widzenia treningu modelu.\n",
    "- Zastanów się jak wyglądałaby Twoja własna implementacja klasy `optim.SGD`.\n",
    "- Prześledź jeszcze raz implementację modelu neuronowego - pewnie w niedługim czasie będziesz implementował znacznie bardziej skomplikowane modele, tym bardziej warto je dobrze prześledzić!\n",
    "- Czym różni się zaimplementowana architektura od głębokiej sieci uśredniającej?\n",
    "- Na wykładzie korzystaliśmy z macierzy zanurzeń w modelach języka. Tutaj warstwa zanurzeń pojawiła się bezpośrednio w modelu klasyfikacji. Czy w uzyskanych w ten sposób zanurzeniach (zakładając dobry dobór hiperparamerów, dodanie regularyzacji itd.) zaobserwowalibyśmy podobne zależności jak te uzyskane za pomocą modelu języka? Jeśli nie, obserwacji jakich zależności między słowami spodziewałbyś się w tej reprezentacji? Skąd biorą się różnice?\n",
    "\n",
    "Odpowiedź na ostatnią kropkę umieść poniżej.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "731e0439a761d52c3124060a972eaf31",
     "grade": true,
     "grade_id": "cell-50c2ecade40bcd7f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Odp.: Niekoniecznie zaobserwujemy podobne zależności w zanurzeniach. Zanurzenia będą się różnić w modelu języka i w klasyfikatorze, bo przy klasyfikacji będą reprezentować przydatne do klasyfikacji, najbardziej różnicujące pomiędzy klasami cechy, a nie jak w modelach języka, odzwierciedlać wybrane zależności między słowami (np. częste współwystępowanie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73080ad0900acee85ddd2dd6ff52a07d",
     "grade": false,
     "grade_id": "cell-a81ef968001b7310",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Zadanie 4\n",
    "Wykorzystując wiedzę z poprzedniego zadania zaimplementuj prostą architekturę splotową do klasyfikacji tekstu i wytrenuj ją. Do jej wykonania może być przydatna klasa `torch.nn.Conv1d` i funkcja `torch.nn.ReLU` (zapoznaj się z ich dokumentacją w Internecie). Jako funkcji redukcji użyj funkcji maksimum (over time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:15:45.093818Z",
     "start_time": "2021-12-01T20:15:36.984091Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2bf89bcb03da956e1b2d2dc56509638",
     "grade": false,
     "grade_id": "cell-dbb714252ae2ebc7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.9517\n",
      "iter 1: avg. train loss=0.9216\n",
      "iter 2: avg. train loss=0.9196\n",
      "iter 3: avg. train loss=0.9146\n",
      "iter 4: avg. train loss=0.9081\n",
      "iter 5: avg. train loss=0.8968\n",
      "iter 6: avg. train loss=0.8818\n",
      "iter 7: avg. train loss=0.8524\n",
      "iter 8: avg. train loss=0.8081\n",
      "iter 9: avg. train loss=0.7171\n",
      "iter 10: avg. train loss=0.5847\n",
      "iter 11: avg. train loss=0.4592\n",
      "iter 12: avg. train loss=0.3375\n",
      "iter 13: avg. train loss=0.2290\n",
      "iter 14: avg. train loss=0.1505\n",
      "iter 15: avg. train loss=0.1018\n",
      "iter 16: avg. train loss=0.0718\n",
      "iter 17: avg. train loss=0.0496\n",
      "iter 18: avg. train loss=0.0315\n",
      "iter 19: avg. train loss=0.0205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (emb): Embedding(1389, 20)\n",
       "  (conv): Conv1d(20, 64, kernel_size=(2,), stride=(1,))\n",
       "  (lin): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, nwords, emb_size, num_filters, window_size, ntags):\n",
    "        super().__init__()\n",
    "        self.emb_size, self.num_filters = emb_size, num_filters\n",
    "        self.emb = torch.nn.Embedding(nwords, emb_size)\n",
    "        self.conv = torch.nn.Conv1d(emb_size, num_filters, kernel_size=window_size)\n",
    "        self.lin = torch.nn.Linear(in_features=num_filters, out_features=n_class, bias=True)\n",
    "        for layer in [self.emb, self.conv, self.lin]:\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        x = words\n",
    "        x = self.emb(x).view(1, self.emb_size, -1)  # -> (batch, emb, time)\n",
    "        x = self.conv(x).view(self.num_filters, -1) # -> (filters, time)\n",
    "        x = F.relu(x).max(1).values                 # -> (filters,)\n",
    "        x = self.lin(x.view(-1))                    # -> (classes,)\n",
    "        return x\n",
    "       \n",
    "def fit(model, epochs=5, lr=0.05):\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        random.shuffle(train)\n",
    "        train_loss = 0.0\n",
    "        for words, tag in train:\n",
    "            pred = model(torch.tensor(words))\n",
    "            loss = F.cross_entropy(pred.view(1,-1), torch.tensor([tag]))\n",
    "            loss.backward()\n",
    "            train_loss += loss\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))\n",
    "    return model\n",
    "        \n",
    "model = CNN(nwords=n_words, emb_size=EMBEDDING_SIZE, ntags=n_class, num_filters=64, window_size=2)\n",
    "fit(model, epochs=20, lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
