{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie danych\n",
    "Paczka Sklearn (Scikit Learn) to kolejne bardzo popularne narzędzie do uczenia maszynowego. Posiada bardzo przejrzyste API i spore wsparcie (scikit-learn.org/). W dzisiejszych zadaniach skupimy się na ładowaniu zbiorów danych, ich transformacji oraz algorytmach klasyfikacji. Podczas dzisiejszych laboratoriów wykorzystamy:\n",
    "<ul>\n",
    "    <li>NLTK - udostępniające metody prostego przetwarzania tekstu (tokenizacja, lematyzacja, stemming)</li>\n",
    "    <li>Sklearn - paczkę do uczenia maszynowego</li>\n",
    "    <li>Pandas - bibliotekę do wczytywania i obsługi zbiorów danych</li>\n",
    "</ul>\n",
    "<span style=\"color: #ff0000\">Ponieważ część kodu jest już stworzona, w każdym zadaniu wyszczególnione są numery linii, w których należy wprowadzić modyfikacje, aby rozwiązać zadanie. Jeśli nie widzisz numeracji linii w kodzie w otwartym notebooku - możesz włączyć tę funkcjonalność poprzez wybór View -> toggle line numbers w górnym menu.</span><br/><br/>\n",
    "Najpierw wczytajmy dane tekstowe ze zbioru, w którym posiadamy zestaw wiadomości e-mail oznaczonych jako spamowe lub niespamowe.\n",
    "Ponieważ będziemy rozwiązywać problem klasyfikacji, oddzielamy dane do trenowania klasyfikatora oraz do weryfikacji jego jakości.\n",
    "<br/>\n",
    "\n",
    "<strong>Przeanalizuj i uruchom poniższy fragment kodu.</strong> Załaduje on odpowiednie dane do dwóch obiektów:\n",
    "<ol>\n",
    "<li>train: zbiór treningowy - dokumenty na których nauczymy klasyfikator</li>\n",
    "<li>test: zbiór testowy - dokumenty na których przetestujemy klasyfikator</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementów w zbiorze treningowym: 1624, testowym: 733\n",
      "\n",
      "\n",
      "Liczność klas w zbiorze treningowym: \n",
      "ham     1111\n",
      "spam     513\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Liczność klas w zbiorze testowym: \n",
      "ham     517\n",
      "spam    216\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Re: What to choose for Core i5 64 bits?&gt;&gt;&gt; If ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Strictly Private.Gooday, With warm heart my fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Re: Flash is open?On Sat, 15 May 2010 00:27:32...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Re: Alsa/Redhat 8 compatabilityMatthias Saou (...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hey hibody, Save 80% today Lixi Eights followi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  label_num\n",
       "0   ham  Re: What to choose for Core i5 64 bits?>>> If ...          0\n",
       "1  spam  Strictly Private.Gooday, With warm heart my fr...          1\n",
       "2   ham  Re: Flash is open?On Sat, 15 May 2010 00:27:32...          0\n",
       "3   ham  Re: Alsa/Redhat 8 compatabilityMatthias Saou (...          0\n",
       "4  spam  Hey hibody, Save 80% today Lixi Eights followi...          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Ładowanie danych i oddzielanie zbioru treningowego od testowego ------\n",
    "\n",
    "try:\n",
    "    full_dataset = pandas.read_csv('spam_emails.csv', encoding='utf-8')      # wczytaj dane z pliku CSV\n",
    "except:\n",
    "    import s3fs\n",
    "    full_dataset = pandas.read_csv(\"https://dwisniewski-put-pjn.s3.eu-north-1.amazonaws.com/spam_emails.csv\")\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham':0, 'spam':1})  # ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\", wykonujemy mapowanie tych wartości na liczby, co będzie potrzebne do wykonania klasyfikacji. \n",
    "\n",
    "np.random.seed(0)                                       # ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7 # wylosuj 70% danych, które stworzą zbiór treningowy. train_indices, to wektor o długości liczności wczytanego zbioru danych, w którym każda pozycja (przykład) może przyjąć dwie wartości: 1.0 - wybierz do zbioru treningowego; 0.0 - wybierz do zbioru testowego\n",
    "\n",
    "train = full_dataset[train_indices].reset_index() # wybierz zbior treningowy (70%)\n",
    "test = full_dataset[~train_indices].reset_index() # wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- Wyświetlanie statystyk -----------------\n",
    "\n",
    "\n",
    "print(\"Elementów w zbiorze treningowym: {train}, testowym: {test}\".format(\n",
    "    train=len(train), test=len(test)\n",
    "))\n",
    "\n",
    "print(\"\\n\\nLiczność klas w zbiorze treningowym: \")\n",
    "print(train.label.value_counts())  # wyświetl rozkład etykiet w kolumnie \"label\"\n",
    "\n",
    "print(\"\\n\\nLiczność klas w zbiorze testowym: \")\n",
    "print(test.label.value_counts())   # wyświetl rozkład etykiet w kolumnie \"label\"\n",
    "\n",
    "\n",
    "\n",
    "full_dataset.head()                # wyświetl próbkę danych\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacja danych\n",
    "Aby zastosować większość algorytmów uczenia maszynowego - dane wejściowe muszą być reprezentowane jako wektory liczb. Wykorzystajmy zatem narzędzia dostarczone przez Scikit-learn do tego celu. Użyjmy klasy **CountVectorizer()**, aby podzielić poszczególne dokumenty na słowa, a następnie stworzyć reprezentację \"bag of words\"\n",
    "Dla przypomnienia - \"bag of words\" tworzony jest w nastepujący sposób:\n",
    "<ol>\n",
    "<li>Przeglądamy wszystkie dostępne dokumenty i tworzymy listę wszystkich unikalnych słów jakie napotkaliśmy (słownik).</li>\n",
    "<li>Stworzona lista wyznacza nam wektor cech - każda pozycja w takim wektorze oznacza jedno z napotkanych słów.</li>\n",
    "<li>Każdy z dokumentów mapowany jest na wektor cech poprzez zapisanie ile razy każde ze słów dokumentu wystąpiło w nim.</li>\n",
    "</ol>\n",
    "Przykład wektoryzacji znajduje się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRZYKŁAD ------------------------------------------\n",
    "# np. Dla dwóch dokumentów:\n",
    "# Dokument 1: Ala ma kota i ma psa \n",
    "# Dokument 2: Kot ma Alę\n",
    "\n",
    "# Poszczególne kroki wyglądają następująco:\n",
    "# Lista unikalnych słów:                 [Ala, ma, kota, i, psa, Kot, Alę] \n",
    "# Szablon wektora cech:                  [  0,  0,    0, 0,   0,   0,   0] - wektor jest tyluelementowy, ile mamy  unikalnych słów \n",
    "# Osadzenie dokumenu 1 jako wektor cech: [  1,  2,    1, 1,   1,   0,   0] - słowo \"ma\" pojawia się w dok. 2 razy, \"kot\" i \"Alę\" - wcale\n",
    "# Analogicznie dokument 2:               [  0,  1,    0, 0,   0,   1,   1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar stworzonej macierzy: (1624, 37325)\n",
      "Liczba dokumentów: 1624\n",
      "Rozmiar wektora bag-of-words 37325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(train['text']) # stwórz macierz liczbową z danych. W wierszach mamy kolejne dokumenty, w kolumnach kolejne pola wektora cech odpowiadające unikalnym słowom (bag of words)\n",
    "X_test_counts = vectorizer.transform(test['text'])       # analogicznie jak wyżej - dla zbioru testowego.\n",
    "\n",
    "print(\"Rozmiar stworzonej macierzy: {x}\".format(x=X_train_counts.shape)) # wyświetl rozmiar macierzy. Pierwsze pole - liczba dokumentów, drugie - liczba cech (stała dla wszystkich dokumentów)\n",
    "print(\"Liczba dokumentów: {x}\".format(x=X_train_counts.shape[0]))\n",
    "print(\"Rozmiar wektora bag-of-words {x}\".format(x=X_train_counts.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uwaga:\n",
    "Na zbiorze treningowym użyto funkcji - fit_transform(), na testowym - transform(). <br/>\n",
    "**Dlaczego?** fit_transform() wykonuje dwie operacje - tworzy i zapisuje listę wszystkich unikalnych słów (słownik) oraz zamienia dokument na wektor o długości takiej jak słownik. transform() natomiast wykorzystuje istniejący już słownik i wykonuje z jego użyciem transformację do wektora. \n",
    "<br/>\n",
    "Ponieważ zbiór treningowy jest zazwyczaj liczniejszy - z reguły znajdziemy w nim więcej różnych słów. Ponadto, wszystkie słowa, które mogą pomóc w klasyfikacji i tak muszą znaleźć się w zbiorze treningowym aby móc się nauczyć ich wykorzystania. Nie nadpisuje się zatem słownika za pomocą zbioru testowego i tworzy się go tylko raz - podczas treningu, wykorzystując go następnie do tworzenia nowych wektorów z nieobserwowanych podczas treningu dokumentów.\n",
    "\n",
    "# Zadanie 1 (1 punkt):\n",
    "Jak się pewnie domyślasz - reprezentacja bag-of-words będzie miała bardzo wiele zer w wygenerowanych macierzach (macierzach, w których w poszczególnych wierszach będziemy mieli poszczególne dokumenty, a w kolumnach wektory słów reprezentacji bag of words). Rozmiar macierzy z poprzedniego listingu pokazuje, że każdy dokument opisany jest wektorem 37325 pozycji, ponieważ tyle różnych słów zostało wykrytych po analizie wszystkich dokumentów treningowych. Większość dokumentów analizowanych osobbno zawierać będzie pewnie co najwyżej kilkadziesiąt/kilkaset różnych słów.\n",
    "\n",
    "***Zadanie: Napisz fragment kodu, który zliczy:***\n",
    "<ol>\n",
    "    <li><strong>jaki procent macierzy X_train_counts ma elementy o wartości różnej od zera</strong></li>\n",
    "    <li><strong>ile tokenów (łącznie, nie tylko unikalne) występuje w macierzy X_train_counts?</strong></li>\n",
    "</ol>\n",
    "Wskazówka - ponieważ zer w tej macierzy jest istotnie dużo - dane po transformacji CountVectorizerem trzymane są w specjalnym formacie, w którym zapisuje się tylko elementy mające wartości różne od zera, w tzw. macierzy rzadkiej (sparse matrix). Aby przeiterować po takiej macierzy, można wykorzystać następujące fragmenty kodu:\n",
    "\n",
    "<strong>cx = X_train_counts.tocoo()</strong> - transformuj macierz do reprezentacji koordynatowej (patrz niżej) <br/>\n",
    "<strong>for doc_id, word_id, count in zip(cx.row, cx.col, cx.data):</strong> - pozwala ona na iterowanie po wszystkich niezerowych elementach, w każdym kroku otrzymując 3 zmienne - numer wiersza (numer dokumentu), numer kolumny (identyfikator słowa ze słownika) oraz licznik mówiący ile razy dane słowo wystąpiło w danym dokumencie. <span style=\"color: #ff0000\">(Do wykonania zadania musisz zaktualizować linijki 3, 7 i 8)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435292 tokenów, 0.394% niezerowych\n"
     ]
    }
   ],
   "source": [
    "M = X_train_counts\n",
    "rows, cols = M.shape\n",
    "count_tokens = M.sum()\n",
    "count_nonzero = (M > 0).sum()\n",
    "count_all = rows * cols\n",
    "print(f'{count_tokens} tokenów, {count_nonzero/count_all*100:.3f}% niezerowych')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <strong>Oczekiwany rezultat:</strong> <br/>\n",
    "Mniej niż 1% elementów niezerowych (!) <br/>\n",
    "Ponad 400000 tokenów\n",
    "</div>\n",
    "\n",
    "# Zadanie 2 (1 punkt) - słowa charakteryzujące klasy\n",
    "\n",
    "Wykorzystajmy macierz X_train_counts wykorzystywaną w poprzednim zadaniu, a także etykiety kategorii, aby stworzyć listy najczęściej występuących słów w danych kategoriach. <br/><br/>\n",
    "Aby ułatwić zadanie, utworzono większość funkcji **get_top_occuring_words()** tworzącej taki ranking<br/>\n",
    "**Zadanie 2a (0.5 punktu)**: twoim zadaniem jest zaktualizowanie wartości pola: **category_word_counts[category][word]**, tak, aby poprawnie zliczyć ile razy dane słowo wystąpiło w kategorii. <span style=\"color: #ff0000\">(zaktualizuj linijkę 20)</span>\n",
    "<br/>\n",
    "Czy najczęstsze słowa pozwalają rozdzielić kategorie SPAM od HAM?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam [('the', 7011), ('of', 4146), ('to', 3594), ('and', 3337), ('in', 2493), ('you', 1749), ('nbsp', 1605), ('for', 1477), ('is', 1328), ('your', 1125), ('this', 1098), ('as', 886)]\n",
      "ham [('the', 10636), ('to', 7555), ('of', 4501), ('and', 4314), ('is', 3519), ('in', 3335), ('it', 3052), ('that', 3044), ('for', 2733), ('you', 2327), ('on', 2257), ('with', 2045)]\n"
     ]
    }
   ],
   "source": [
    "def print_counts(X, data, vectorizer, top=12):\n",
    "    id_to_word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "    for category in ['spam', 'ham']:\n",
    "        idx = data[data.label == category].index\n",
    "        counts = np.array(X[idx,:].sum(axis=0))[0]\n",
    "        words = [id_to_word[i] for i in range(len(counts))]\n",
    "        word_counts = sorted(zip(words, counts), key=lambda x: x[1], reverse=True)\n",
    "        print(category, word_counts[:top])\n",
    "    \n",
    "print_counts(X_train_counts, train, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wektoryzacja Tf-Idf\n",
    "\n",
    "Po wykonaniu zadania 2a widzimy, że najczęściej występujące słowa w każdej kategorii mają niewielką użytezczność (pasują do każdej kategorii). Aby sprawić, żeby na czele rankingu znalazły się słowa charakterystyczne dla danej klasy, możemy użyć metody Tf-idf. <br/>\n",
    "**Zadanie 2b:\n",
    "Nadpisz wartości X_train_counts oraz X_test_counts wykorzystując w tym celu TfidfVectorizer** (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) zamiast CountVectorizer, ustaw parametr max_df na 0.4 (tzn. ignoruj słowa, które występują w więcej niż 40% dokumentów). Następnie wykonaj stworzoną w zadaniu 2 funkcję get_top_occuring_words(), aby sprawdzić, czy ranking najważniejszych słów się zmienił. Czy zmieniony zestaw słów lepiej reprezentuje kategorie? <span style=\"color: #ff0000\">(zaktualizuj linie 3, 4, 5)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.4)\n",
    "X_train_counts = vectorizer.fit_transform(train.text)\n",
    "X_test_counts = vectorizer.transform(test.text)\n",
    "\n",
    "print_counts(X_train_counts, train, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3 - Stemming i lematyzacja (1 punkt)\n",
    "Często istotne słowa występują w wielu odmianach (szczególnie w językach fleksyjnych, takich jak nasz), np: university - universities ; pay - paid - paying - pays . Wielość odmian słów ma swoje przełożenie na rozmiar słownika.\n",
    "<br/><br/>\n",
    "W niektórych warunkach, w szczególności:\n",
    "<ul>\n",
    "<li>Kiedy mamy ograniczoną pamięć</li>\n",
    "<li>Kiedy ważny jest dla nas czas działania algorytmu</li>\n",
    "<li>Kiedy istnieje ryzyko przeuczenia</li>\n",
    "</ul>\n",
    "warto rozważyć znormalizowanie słów, tak, aby zmniejszyć rozmiar słownika, a co za tym idzie wymagania pamięciowe (a co za tym idzie - czas treningu/klasyfikacji). Ograniczenie rozmiaru słownika może też zapobiec przeuczeniu. Normalizację możemy wykonać np. poprzez zastosowanie stemmingu lub lematyzacji dla poszczególnych wyrazów.\n",
    "<br/>\n",
    "<strong>Zadanie 3a (0.5 punktu)</strong>: Z użyciem biblioteki NLTK wykonaj zarówno lematyzację (używając WordNetLemmatizer) jak i stemming (używając PorterStemmer) tekstu zawartego w sample_text. Uwaga - lematyzator opcjonalnie wymaga pos-tagu dla tokenu. Przekaż do funkcji lematyzującej zmienną current_word_postag jako drugi argument. <span style=\"color: #ff0000\">(zaktualizuj linie 20, 21, 34, 35)</span>\n",
    "\n",
    "<strong>Zadanie 3b (0.5 punktu)</strong>: O ile zmniejszyła się liczba unikalnych słów po zastosowaniu lematyzacji? Odpowiedź zawrzyj w komentarzu. <span style=\"color: #ff0000\">(linijki 43:45)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):     # lematyzator wymaga, aby dla danego słowa podać mu, czy jest to czasownik, rzeczownik czy inny POS-tag. Funkcja jest adapterem tagów nadanych przez funkcję pos_tag do tagów wymaganych przez lematyzator \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "sample_text = \"There are some cheaper alternatives for buying the red trousers. There is a discount, it is so cheap!\"\n",
    "\n",
    "lemmatized = [] # tutaj będziemy dopisywać zlematyzowane słowa\n",
    "stemmed = []    # tutaj będziemy dopisywać wystemowane słowa\n",
    "\n",
    "tokenized = word_tokenize(sample_text)     # dzielimy tekst na słowa\n",
    "pos_tokens = nltk.pos_tag(tokenized)       # nadajemy pos-tagi (rzeczownik, czasownik przymiotnik...) każdemu słowu\n",
    "\n",
    "\n",
    "for i in range(len(tokenized)): # dla każdego słowa\n",
    "    pos = get_wordnet_pos(pos_tokens[i][1]) # pobieramy pos-tag słowa\n",
    "    word = tokenized[i]\n",
    "    lemmatized_token = wordnet_lemmatizer.lemmatize(word.lower(), pos=pos)\n",
    "    stemmed_token = porter_stemmer.stem(word.lower()) \n",
    "    \n",
    "    lemmatized.append(lemmatized_token)\n",
    "    stemmed.append(stemmed_token)\n",
    "print(f\"Bazowy tekst:        {sample_text}\")\n",
    "print(f\"Wystemowany tekst:   {' '.join(stemmed)}\")\n",
    "print(f\"Zlematyzowany tekst: {' '.join(lemmatized)}\")\n",
    "\n",
    "base_count, lemma_count = len(set(tokenized)), len(set(lemmatized))\n",
    "print(f'unikalne w tekście bazowym: {base_count}')\n",
    "print(f'unikalne po lematyzacji: {lemma_count}')\n",
    "print(f'difference: {base_count - lemma_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Zadanie 4 (1 punkt) - klasyfikacja i interpretacja wyników\n",
    "Mając już dobrą reprezentację danych i wiedząc jak działa normalizacja - możemy klasyfikować! <br/>\n",
    "Istnieje wiele algorytmów, które dobrze radzą sobie z klasyfikacją tekstu, kilka przykładów to: \n",
    "<ul>\n",
    "<li>Naiwny klasyfikator Bayesa</li>\n",
    "<li>Maszyna wektorów nośnych - SVM</li>\n",
    "<li>Sieci neuronowe</li>\n",
    "</ul>\n",
    "O sieciach neuronowych więcej powiemy na jednych z przyszłych laboratoriów. <br/>\n",
    "<strong>Zadanie 4a (0.5 punktu)</strong> Wykorzystując przetworzoną postać danych: X_train_counts, X_test_counts z poprzednich zadań oraz dokumentację sklearn, zaimplementuj klasyfikację z użyciem naiwnego klasyfikatora Bayesa (MultinomialNB). <span style=\"color: #ff0000\">(zaktualizuj linie 7, 9, 12)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def labels_as_strings(vector_of_indices): # funkcja pomocnicza zamieniająca identyfikatory numeryczne na tekstowe\n",
    "    return ['ham' if ind == 0 else 'spam' for ind in vector_of_indices] \n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_counts, train.label_num)\n",
    "\n",
    "print(\"Ile elementów testowych udało się poprawnie zaklasyfikować?\")\n",
    "accuracy = nb.score(X_test_counts, test.label_num)\n",
    "print(f'accuracy = {accuracy:.4f}')\n",
    "print(\"Szczegółowy raport (per klasa)\")\n",
    "print(classification_report(labels_as_strings(test['label_num']), labels_as_strings(nb.predict(X_test_counts))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 4b (0.5 punktu)\n",
    "Po analizie szczegółowego raportu z zadania 4a - odpowiedz na poniższe pytania i zapisz odpowiedzi w komentarzu:**\n",
    "<ol>\n",
    "<li>Która miara mówi nam o tym, jak wiele spośród elementów uznanych za spam rzeczywiście jest spamem?</li>\n",
    "<li>Która miara mówi nam o tym, jak wiele spośród wszystkich elementów rzeczywiście będących spamem zostało wykrytych jako spam?</li>\n",
    "<li>Która kategoria została w ogólnym rozrachunku lepiej rozpoznana przez klasyfikator, jeśli zależy nam bardziej na tym, żeby klasyfikator, jeśli mówi, że coś należy do danej klasy, raczej się w tym nie mylił, niż żeby wykrył wszystkie elementy klasy?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odp zad 4.1: precision\n",
    "# odp zad 4.2: recall\n",
    "# odp zad 4.3: precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn jest bardzo wdzięcznym narzędziem, w którym proces klasyfikacji możemy wykonać w zaledwie kilku linijkach. Bardzo przydatną klasą jest klasa Pipeline, która definiuje sekwencję kroków, które wykonujemy wywołując metodę fit().\n",
    "W naszym przypadku mamy dwa kroki:\n",
    "<ol>\n",
    "    <li>Wektoryzacja - zamienia dane zapisane w postaci tekstowej na macierz z wektorami bag-of-words.</li>\n",
    "    <li>Klasyfikacja - wytrenowanie klasyfikatora.</li>\n",
    "</ol>\n",
    "W zdefiniowanym obiekcie typu pipeline, i+1 element pipeline'u na wejściu dostaje dane z wyjścia i-tego elementu (Zatem nasz klasyfikator otrzyma dane przetworzone przez TfidfVectorizer). <br/>\n",
    "Metoda fit na wejściu przyjmuje listę dokumentów w formie tekstowej, oraz oczekiwane etykiety w formie liczbowej.\n",
    "<br/>\n",
    "Analogicznie w procesie klasyfikowania nowych tekstów z użyciem istniejącego modelu - metoda predict() wykona sekwencję kroków: wektoryzacja + klasyfikacja dla zadanej listy surowych tekstów). <br/>\n",
    "Zapoznaj się z poniższym kodem i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- WCZYTANIE DANYCH -----------\n",
    "\n",
    "dataset = pd.read_csv('spam_emails.csv', encoding='utf-8')\n",
    "dataset['label_num'] = dataset.label.map({'ham':0, 'spam':1})\n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=0) \n",
    "\n",
    "\n",
    "# ------------------- STWORZENIE PIPELINE'U -----------\n",
    "    \n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_df=0.4)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
    "\n",
    "pipeline.fit(train.text, train.label_num)\n",
    "\n",
    "# ------------------- KLASYFIKACJA PRZYKŁADOWEGO TEKSTU -----------\n",
    "\n",
    "text_to_predict = \"NEED TO FIND SOMETHING? ::FREE MORTGAGE QUOTE:: To be removed from this list, click here. \"\n",
    "predicted = pipeline.predict([text_to_predict])\n",
    "detected = 'SPAM' if predicted == 1 else 'HAM'\n",
    "print(f\"Tekst {text_to_predict}, zaklasyfikowany został jako: {detected}\")\n",
    "\n",
    "# ------------------- OCENA KLASYFIKATORA -----------\n",
    "accuracy = pipeline.score(test.text, test.label_num)\n",
    "print(f\"W zbiorze testowym {100*accuracy:.2f}% przypadków zostało poprawnie zaklasyfikowanych!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zadanie 5 (1 punkt): dobór parametrów klasyfikacji\n",
    "Poniżej znajduje się kod tworzący pipeline składający się z dwóch elementów: TfidfVectorizera oraz klasyfikatora naiwnego Bayesa - MultinomialNB. Wektoryzator tworzy model bag-of-words, który uwzględnia jedynie 1000 najważniejszych słów w słowniku. W celu zastosowania stemmingu oraz lematyzatora w treningu i predykcji stoworzona została klasa TheTokenizer, która poza podziałem tekstu na słowa wykonuje również zadania normalizacji wg. ustalonych flag: **use_stemming, use_lemmatization, use_stopword_removal**. <br/>\n",
    "<strong>Zadanie 5a (0.5 punktu)</strong>: <br/>\n",
    "Zweryfikuj jak zmiana wartości flag **use_stemming, use_lemmatization, use_stopword_removal**, a co za tym idzie wykorzystanie lamatyzacji, stemmingu i usuwania najczęstszych słow wpływa na miary precision, recall i f1 stworzonego klasyfikatora. Wyniki zapisz w komentarzu. <span style=\"color: #ff0000\">(modyfikuj linie 16, 17, 18, komentarz - w kolejnej komórce)</span><br/>\n",
    "<strong> Zadanie 5b (0.5 punktu)</strong>: <br/>\n",
    "Ustaw flagi **use_stemming, use_lemmatization, use_stopword_removal** z linii 16,17 i 18 na False, i porównaj wartości precision recall i f1 dla klasyfikatora, ktory wykorzystuje CountVectorizer i takiego, który wykorzystuje TfidfVectorizer. Pozostaw parametr max_features=1000 w obu przypadkach. Który wektoryzator jest lepszy? <span style=\"color: #ff0000\">(modyfikuj linię 84)</span>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pobieranie danych\n",
      "Tworzenie pipeline'u\n",
      "Uczenie pipeline'u\n",
      "[('your', 999), ('young', 998), ('you', 997), ('york', 996), ('yet', 995), ('yes', 994), ('years', 993), ('year', 992), ('wrote', 991), ('wrong', 990), ('written', 989), ('writes', 988), ('write', 987), ('wouldn', 986), ('would', 985), ('worth', 984), ('world', 983), ('works', 982), ('working', 981), ('work', 980), ('words', 979), ('word', 978), ('won', 977), ('women', 976), ('without', 975), ('within', 974), ('with', 973), ('wire', 972), ('windows', 971), ('will', 970), ('why', 969), ('whole', 968), ('who', 967), ('white', 966), ('while', 965), ('which', 964), ('whether', 963), ('where', 962), ('when', 961), ('whatever', 960), ('what', 959), ('were', 958), ('went', 957), ('well', 956), ('weeks', 955), ('week', 954), ('weapons', 953), ('weapon', 952), ('we', 951), ('way', 950), ('water', 949), ('wasn', 948), ('washington', 947), ('was', 946), ('war', 945), ('wanted', 944), ('want', 943), ('vs', 942), ('virginia', 941), ('view', 940), ('video', 939), ('via', 938), ('very', 937), ('version', 936), ('ve', 935), ('various', 934), ('uunet', 933), ('uucp', 932), ('utexas', 931), ('usually', 930), ('using', 929), ('uses', 928), ('users', 927), ('user', 926), ('usenet', 925), ('used', 924), ('use', 923), ('usa', 922), ('us', 921), ('upon', 920), ('up', 919), ('until', 918), ('unless', 917), ('unix', 916), ('university', 915), ('univ', 914), ('united', 913), ('understand', 912), ('under', 911), ('uk', 910), ('uiuc', 909), ('udel', 908), ('type', 907), ('two', 906), ('tv', 905), ('turn', 904), ('trying', 903), ('try', 902), ('truth', 901), ('true', 900), ('tried', 899), ('treatment', 898), ('total', 897), ('toronto', 896), ('top', 895), ('took', 894), ('too', 893), ('tom', 892), ('told', 891), ('together', 890), ('today', 889), ('to', 888), ('tin', 887), ('times', 886), ('time', 885), ('ti', 884), ('thus', 883), ('through', 882), ('three', 881), ('thought', 880), ('though', 879), ('those', 878), ('thomas', 877), ('this', 876), ('thinking', 875), ('think', 874), ('things', 873), ('thing', 872), ('they', 871), ('these', 870), ('therefore', 869), ('there', 868), ('theory', 867), ('then', 866), ('themselves', 865), ('them', 864), ('their', 863), ('the', 862), ('that', 861), ('thanks', 860), ('thank', 859), ('than', 858), ('texas', 857), ('test', 856), ('term', 855), ('tell', 854), ('technology', 853), ('tax', 852), ('talking', 851), ('talk', 850), ('taking', 849), ('taken', 848), ('take', 847), ('systems', 846), ('system', 845), ('surface', 844), ('sure', 843), ('support', 842), ('supply', 841), ('sun', 840), ('summary', 839), ('suggest', 838), ('such', 837), ('subject', 836), ('stuff', 835), ('study', 834), ('studies', 833), ('student', 832), ('street', 831), ('stratus', 830), ('story', 829), ('stop', 828), ('still', 827), ('steve', 826), ('stephanopoulos', 825), ('station', 824), ('states', 823), ('statement', 822), ('state', 821), ('started', 820), ('start', 819), ('stanford', 818), ('standard', 817), ('stand', 816), ('st', 815), ('speed', 814), ('specific', 813), ('special', 812), ('speak', 811), ('space', 810), ('south', 809), ('sources', 808), ('source', 807), ('sound', 806), ('sort', 805), ('sorry', 804), ('soon', 803), ('son', 802), ('sometimes', 801), ('something', 800), ('someone', 799), ('some', 798), ('software', 797), ('society', 796), ('so', 795), ('smith', 794), ('small', 793), ('size', 792), ('situation', 791), ('site', 790), ('single', 789), ('since', 788), ('sin', 787), ('simply', 786), ('simple', 785), ('similar', 784), ('side', 783), ('shuttle', 782), ('show', 781), ('should', 780), ('short', 779), ('shipping', 778), ('she', 777), ('shall', 776), ('several', 775), ('set', 774), ('services', 773), ('service', 772), ('serious', 771), ('sense', 770), ('send', 769), ('sell', 768), ('self', 767), ('seen', 766), ('seems', 765), ('seem', 764), ('see', 763), ('section', 762), ('second', 761), ('se', 760), ('scsi', 759), ('screen', 758), ('scott', 757), ('scientific', 756), ('science', 755), ('sci', 754), ('school', 753), ('says', 752), ('saying', 751), ('say', 750), ('saw', 749), ('satellite', 748), ('san', 747), ('same', 746), ('sale', 745), ('said', 744), ('safety', 743), ('rutgers', 742), ('running', 741), ('run', 740), ('rocket', 739), ('rochester', 738), ('robert', 737), ('road', 736), ('rights', 735), ('right', 734), ('ride', 733), ('results', 732), ('result', 731), ('rest', 730), ('response', 729), ('research', 728), ('required', 727), ('require', 726), ('report', 725), ('reply', 724), ('remember', 723), ('religion', 722), ('related', 721), ('reference', 720), ('recently', 719), ('reasonable', 718), ('reason', 717), ('really', 716), ('real', 715), ('reading', 714), ('read', 713), ('re', 712), ('ray', 711), ('rather', 710), ('rate', 709), ('range', 708), ('ram', 707), ('radio', 706), ('quite', 705), ('questions', 704), ('question', 703), ('quality', 702), ('put', 701), ('purpose', 700), ('public', 699), ('pub', 698), ('provide', 697), ('project', 696), ('programs', 695), ('program', 694), ('products', 693), ('process', 692), ('problems', 691), ('problem', 690), ('probably', 689), ('pro', 688), ('private', 687), ('printer', 686), ('price', 685), ('pretty', 684), ('press', 683), ('president', 682), ('present', 681), ('power', 680), ('posting', 679), ('posted', 678), ('post', 677), ('possible', 676), ('position', 675), ('political', 674), ('police', 673), ('points', 672), ('point', 671), ('plus', 670), ('please', 669), ('plan', 668), ('place', 667), ('pittsburgh', 666), ('pitt', 665), ('picture', 664), ('phone', 663), ('peter', 662), ('personal', 661), ('person', 660), ('perhaps', 659), ('per', 658), ('people', 657), ('pc', 656), ('pay', 655), ('paul', 654), ('patients', 653), ('pat', 652), ('past', 651), ('parts', 650), ('particular', 649), ('part', 648), ('pain', 647), ('page', 646), ('package', 645), ('own', 644), ('over', 643), ('outside', 642), ('output', 641), ('out', 640), ('our', 639), ('otherwise', 638), ('others', 637), ('other', 636), ('original', 635), ('organization', 634), ('org', 633), ('order', 632), ('orbit', 631), ('or', 630), ('optilink', 629), ('opinions', 628), ('opinion', 627), ('open', 626), ('only', 625), ('online', 624), ('ones', 623), ('one', 622), ('once', 621), ('on', 620), ('old', 619), ('ok', 618), ('ohio', 617), ('oh', 616), ('often', 615), ('office', 614), ('offer', 613), ('off', 612), ('of', 611), ('numbers', 610), ('number', 609), ('nuclear', 608), ('now', 607), ('nothing', 606), ('note', 605), ('not', 604), ('north', 603), ('normal', 602), ('nor', 601), ('non', 600), ('no', 599), ('nntp', 598), ('night', 597), ('nice', 596), ('next', 595), ('newsreader', 594), ('newsgroup', 593), ('news', 592), ('new', 591), ('never', 590), ('network', 589), ('netcom', 588), ('net', 587), ('needs', 586), ('needed', 585), ('need', 584), ('necessary', 583), ('nec', 582), ('near', 581), ('nature', 580), ('national', 579), ('nasa', 578), ('name', 577), ('na', 576), ('myself', 575), ('my', 574), ('must', 573), ('much', 572), ('msg', 571), ('ms', 570), ('mr', 569), ('most', 568), ('more', 567), ('moon', 566), ('months', 565), ('monitor', 564), ('money', 563), ('modem', 562), ('model', 561), ('mode', 560), ('mit', 559), ('mission', 558), ('mine', 557), ('mind', 556), ('militia', 555), ('military', 554), ('mil', 553), ('mike', 552), ('might', 551), ('michael', 550), ('message', 549), ('mentioned', 548), ('men', 547), ('memory', 546), ('members', 545), ('medicine', 544), ('medical', 543), ('media', 542), ('means', 541), ('mean', 540), ('me', 539), ('maybe', 538), ('may', 537), ('matter', 536), ('mass', 535), ('market', 534), ('mark', 533), ('many', 532), ('man', 531), ('making', 530), ('makes', 529), ('make', 528), ('major', 527), ('main', 526), ('mail', 525), ('made', 524), ('machine', 523), ('mac', 522), ('ma', 521), ('lunar', 520), ('low', 519), ('love', 518), ('lot', 517), ('lost', 516), ('lord', 515), ('looking', 514), ('look', 513), ('longer', 512), ('long', 511), ('local', 510), ('ll', 509), ('living', 508), ('live', 507), ('little', 506), ('list', 505), ('lines', 504), ('line', 503), ('likely', 502), ('like', 501), ('light', 500), ('life', 499), ('library', 498), ('level', 497), ('let', 496), ('less', 495), ('legal', 494), ('left', 493), ('least', 492), ('laws', 491), ('law', 490), ('launch', 489), ('later', 488), ('last', 487), ('large', 486), ('laboratory', 485), ('lab', 484), ('known', 483), ('knowledge', 482), ('know', 481), ('king', 480), ('kind', 479), ('keywords', 478), ('keyboard', 477), ('keep', 476), ('just', 475), ('jpl', 474), ('jpeg', 473), ('john', 472), ('jobs', 471), ('job', 470), ('jim', 469), ('jesus', 468), ('james', 467), ('itself', 466), ('its', 465), ('it', 464), ('issues', 463), ('issue', 462), ('isn', 461), ('isc', 460), ('is', 459), ('involved', 458), ('into', 457), ('internet', 456), ('international', 455), ('internal', 454), ('interesting', 453), ('interested', 452), ('interest', 451), ('insurance', 450), ('institute', 449), ('instead', 448), ('inside', 447), ('input', 446), ('information', 445), ('info', 444), ('including', 443), ('includes', 442), ('included', 441), ('include', 440), ('inc', 439), ('in', 438), ('important', 437), ('images', 436), ('image', 435), ('ii', 434), ('if', 433), ('ideas', 432), ('idea', 431), ('ibm', 430), ('human', 429), ('hp', 428), ('however', 427), ('how', 426), ('house', 425), ('host', 424), ('hope', 423), ('home', 422), ('hold', 421), ('history', 420), ('his', 419), ('him', 418), ('higher', 417), ('high', 416), ('hi', 415), ('here', 414), ('her', 413), ('henry', 412), ('help', 411), ('hell', 410), ('heart', 409), ('heard', 408), ('hear', 407), ('health', 406), ('head', 405), ('he', 404), ('having', 403), ('haven', 402), ('have', 401), ('has', 400), ('harvard', 399), ('hardware', 398), ('hard', 397), ('happened', 396), ('happen', 395), ('hand', 394), ('half', 393), ('had', 392), ('guy', 391), ('guns', 390), ('gun', 389), ('guess', 388), ('groups', 387), ('group', 386), ('ground', 385), ('great', 384), ('graphics', 383), ('government', 382), ('gov', 381), ('got', 380), ('gordon', 379), ('good', 378), ('going', 377), ('goes', 376), ('god', 375), ('go', 374), ('gmt', 373), ('given', 372), ('give', 371), ('gif', 370), ('getting', 369), ('gets', 368), ('get', 367), ('georgia', 366), ('general', 365), ('geb', 364), ('gas', 363), ('gary', 362), ('game', 361), ('future', 360), ('further', 359), ('full', 358), ('ftp', 357), ('front', 356), ('from', 355), ('friend', 354), ('free', 353), ('four', 352), ('found', 351), ('format', 350), ('form', 349), ('force', 348), ('for', 347), ('food', 346), ('following', 345), ('flight', 344), ('first', 343), ('firearms', 342), ('fire', 341), ('fine', 340), ('find', 339), ('files', 338), ('file', 337), ('figure', 336), ('few', 335), ('feel', 334), ('federal', 333), ('fbi', 332), ('fax', 331), ('father', 330), ('fast', 329), ('far', 328), ('faq', 327), ('family', 326), ('faith', 325), ('fact', 324), ('face', 323), ('explain', 322), ('experience', 321), ('expect', 320), ('exist', 319), ('except', 318), ('excellent', 317), ('example', 316), ('exactly', 315), ('evidence', 314), ('everything', 313), ('everyone', 312), ('every', 311), ('ever', 310), ('even', 309), ('etc', 308), ('especially', 307), ('equipment', 306), ('enough', 305), ('engineering', 304), ('eng', 303), ('end', 302), ('email', 301), ('else', 300), ('either', 299), ('effect', 298), ('education', 297), ('edu', 296), ('ed', 295), ('easy', 294), ('east', 293), ('earth', 292), ('early', 291), ('each', 290), ('during', 289), ('due', 288), ('drugs', 287), ('drug', 286), ('drive', 285), ('dr', 284), ('down', 283), ('dos', 282), ('done', 281), ('don', 280), ('doing', 279), ('doesn', 278), ('does', 277), ('dod', 276), ('doctor', 275), ('do', 274), ('division', 273), ('distribution', 272), ('display', 271), ('disk', 270), ('disease', 269), ('discussion', 268), ('disclaimer', 267), ('digex', 266), ('different', 265), ('difference', 264), ('die', 263), ('didn', 262), ('did', 261), ('development', 260), ('details', 259), ('designed', 258), ('design', 257), ('dept', 256), ('department', 255), ('defense', 254), ('death', 253), ('deal', 252), ('dead', 251), ('de', 250), ('dc', 249), ('days', 248), ('day', 247), ('david', 246), ('dave', 245), ('date', 244), ('data', 243), ('dan', 242), ('cwru', 241), ('cut', 240), ('currently', 239), ('current', 238), ('cso', 237), ('cs', 236), ('crime', 235), ('cramer', 234), ('cover', 233), ('court', 232), ('course', 231), ('couple', 230), ('country', 229), ('could', 228), ('cost', 227), ('correct', 226), ('corporation', 225), ('corp', 224), ('copy', 223), ('control', 222), ('contact', 221), ('considered', 220), ('consider', 219), ('congress', 218), ('condition', 217), ('computing', 216), ('computer', 215), ('complete', 214), ('company', 213), ('comp', 212), ('communications', 211), ('common', 210), ('commercial', 209), ('coming', 208), ('comes', 207), ('come', 206), ('com', 205), ('columbia', 204), ('colorado', 203), ('color', 202), ('college', 201), ('code', 200), ('co', 199), ('cmu', 198), ('close', 197), ('clinton', 196), ('cleveland', 195), ('clear', 194), ('class', 193), ('claim', 192), ('city', 191), ('circuit', 190), ('church', 189), ('christians', 188), ('christianity', 187), ('christian', 186), ('christ', 185), ('chris', 184), ('chip', 183), ('children', 182), ('child', 181), ('check', 180), ('cheap', 179), ('change', 178), ('certainly', 177), ('certain', 176), ('center', 175), ('cd', 174), ('cc', 173), ('cause', 172), ('cases', 171), ('case', 170), ('carry', 169), ('care', 168), ('card', 167), ('car', 166), ('cannot', 165), ('canada', 164), ('can', 163), ('came', 162), ('called', 161), ('call', 160), ('california', 159), ('cable', 158), ('ca', 157), ('by', 156), ('buy', 155), ('but', 154), ('business', 153), ('built', 152), ('build', 151), ('box', 150), ('both', 149), ('books', 148), ('book', 147), ('body', 146), ('board', 145), ('bnr', 144), ('black', 143), ('bitnet', 142), ('bit', 141), ('billion', 140), ('bill', 139), ('bike', 138), ('big', 137), ('bible', 136), ('between', 135), ('better', 134), ('best', 133), ('berkeley', 132), ('believe', 131), ('being', 130), ('before', 129), ('been', 128), ('become', 127), ('because', 126), ('be', 125), ('bbs', 124), ('batf', 123), ('basic', 122), ('based', 121), ('banks', 120), ('bad', 119), ('back', 118), ('away', 117), ('available', 116), ('authority', 115), ('austin', 114), ('audio', 113), ('au', 112), ('att', 111), ('at', 110), ('assume', 109), ('asking', 108), ('asked', 107), ('ask', 106), ('as', 105), ('article', 104), ('around', 103), ('arms', 102), ('argument', 101), ('area', 100), ('are', 99), ('april', 98), ('apr', 97), ('apple', 96), ('appears', 95), ('anyway', 94), ('anything', 93), ('anyone', 92), ('anybody', 91), ('any', 90), ('anti', 89), ('answer', 88), ('another', 87), ('andy', 86), ('andrew', 85), ('and', 84), ('analysis', 83), ('an', 82), ('amount', 81), ('among', 80), ('american', 79), ('america', 78), ('amendment', 77), ('am', 76), ('always', 75), ('although', 74), ('also', 73), ('already', 72), ('along', 71), ('almost', 70), ('all', 69), ('alaska', 68), ('al', 67), ('air', 66), ('ai', 65), ('agree', 64), ('ago', 63), ('age', 62), ('against', 61), ('again', 60), ('after', 59), ('advice', 58), ('advance', 57), ('address', 56), ('add', 55), ('actually', 54), ('action', 53), ('act', 52), ('acs', 51), ('according', 50), ('access', 49), ('accept', 48), ('ac', 47), ('above', 46), ('about', 45), ('able', 44), ('___', 43), ('__', 42), ('93', 41), ('92', 40), ('90', 39), ('800', 38), ('80', 37), ('60', 36), ('50', 35), ('45', 34), ('41', 33), ('40', 32), ('3d', 31), ('35', 30), ('33', 29), ('32', 28), ('31', 27), ('30', 26), ('29', 25), ('28', 24), ('27', 23), ('26', 22), ('25', 21), ('24', 20), ('23', 19), ('22', 18), ('21', 17), ('20', 16), ('1993apr15', 15), ('1993', 14), ('1992', 13), ('19', 12), ('18', 11), ('17', 10), ('16', 9), ('15', 8), ('14', 7), ('13', 6), ('12', 5), ('11', 4), ('100', 3), ('10', 2), ('000', 1), ('00', 0)]\n",
      "Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika?)?\n",
      "W słowniku znajduje się 1000 różnych słów\n",
      "Ocena klasyfikatora\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.71      0.73      0.72       389\n",
      " comp.sys.mac.hardware       0.71      0.78      0.74       385\n",
      "          misc.forsale       0.79      0.92      0.85       390\n",
      "       rec.motorcycles       0.64      0.90      0.74       398\n",
      "       sci.electronics       0.65      0.59      0.62       393\n",
      "               sci.med       0.79      0.60      0.68       396\n",
      "             sci.space       0.87      0.77      0.81       394\n",
      "soc.religion.christian       0.94      0.87      0.90       398\n",
      "    talk.politics.guns       0.68      0.80      0.73       364\n",
      "    talk.politics.misc       0.78      0.45      0.57       310\n",
      "\n",
      "              accuracy                           0.75      3817\n",
      "             macro avg       0.75      0.74      0.74      3817\n",
      "          weighted avg       0.76      0.75      0.74      3817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wczytywanie danych\n",
    "from sklearn.datasets import fetch_20newsgroups # zbiór danych zawarty w Sklearn, który zawiera dane z 20 grup newsowych\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk import word_tokenize, pos_tag       \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def labels_as_strings(vector_of_indices):\n",
    "    return [dataset_train.target_names[ind] for ind in vector_of_indices] \n",
    "\n",
    "print(\"Pobieranie danych\")\n",
    "categories = ['misc.forsale', 'soc.religion.christian', 'sci.space', 'talk.politics.guns',\n",
    "              'comp.graphics', 'sci.med',  'rec.motorcycles',  'sci.med',\n",
    "              'sci.electronics', 'talk.politics.misc', 'comp.sys.mac.hardware']\n",
    "dataset_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "dataset_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Tworzenie pipeline'u\")\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    #('vectorizer', TfidfVectorizer(tokenizer=TheTokenizer(), max_features=1000)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Uczenie pipeline'u\")\n",
    "pipeline.fit(dataset_train.data, dataset_train.target) # trenujemy klasyfikator!\n",
    "\n",
    "print(sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika?)?\")\n",
    "print(\"W słowniku znajduje się {n} różnych słów\".format(\n",
    "    n=len(pipeline.named_steps['vectorizer'].vocabulary_.keys())\n",
    "))\n",
    "\n",
    "\n",
    "print(\"Ocena klasyfikatora\")\n",
    "print(classification_report(labels_as_strings(dataset_test.target), labels_as_strings(pipeline.predict(dataset_test.data)))) # testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "text = ['ala ma kota a kot ma alę']\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 1000 - CountVectorizer\n",
    "        Precision: 0.75  Recall: 0.74   F1: 0.74\n",
    "\n",
    "Top 1000 - TfIDF:\n",
    "    Bez usuwania stopwords:\n",
    "        Precision: 0.74  Recall: 0.72   F1: 0.72\n",
    "    Usuwanie stopwords: \n",
    "        Precision: 0.76  Recall: 0.74   F1: 0.74\n",
    "    Usuwanie stopwords + stemming:   \n",
    "        Precision: 0.79  Recall: 0.77   F1: 0.77\n",
    "    Usuwanie stopwords + lematyzacja:\n",
    "        Precision: 0.79  Recall: 0.78   F1: 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
